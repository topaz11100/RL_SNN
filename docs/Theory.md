## 1. 연구 개요

### 1.1 문제 설정

이 문서는 시냅스 가중치 업데이트를 **로컬 정책(가중치 정책, weight policy)** 으로 보고 이를 **강화학습(Actor–Critic)** 으로 학습시키는 SNN 실험 설계를 정리한다. 전체 내용은 실제 코드 구현이 지향하는 **이상적인 알고리즘 설계**를 기준으로 한다.

전통적인 **STDP** 는 pre/post 스파이크의 시간차만을 보고 고정된 규칙에 따라 가중치 변화를 결정하는 완전히 하드코딩된 학습 규칙이다. 이 프로젝트에서는 다음과 같이 가정한다.

* 각 시냅스는 로컬 스파이크 히스토리, 현재 가중치 등의 정보를 기본 입력으로 받는다. (완전지도 실험에서는 레이어 위치 정보도 추가로 포함한다.)
* 이 로컬 상태를 입력으로 받아 **실수 스칼라 액션(정규화된 가중치 변화량)** 을 출력하는 **Gaussian Actor** 를 둔다.
* 동일한 로컬 상태를 입력으로 받아 **기대 누적 보상(baseline)** 을 추정하는 Critic 을 둔다.
* 즉 시냅스 업데이트는 고정 STDP 규칙이 아니라 **로컬 상태 → 실수 액션** 을 내는 작은 RL 에이전트들의 집합으로 모델링된다. 파라미터는 모든 시냅스 간에 공유되지만, 입력은 각 시냅스의 로컬 스파이크 히스토리이므로 **“단일 뉴런/단일 시냅스 에이전트의 근사”** 로 해석할 수 있다.

이 설계를 기반으로 다음 네 가지 실험 시나리오를 비교·분석한다.

1. 완전 비지도 단일 정책 (시나리오 1.1)
2. 완전 비지도 두 정책 (시나리오 1.2)
3. 준지도 단일 정책 분류 (시나리오 2)
4. 완전지도 gradient mimicry (시나리오 3)

네 시나리오는 **동일한 로컬 상태, CNN+MLP 구조, PPO 기반 Actor–Critic 업데이트** 를 공유하되

* 사용하는 SNN 아키텍처
* 정책의 개수(1개 vs 2개)
* 전역 보상(완전 비지도 vs 준지도 분류 vs gradient 기반)

에서 서로 다르게 설계된다.

강화학습 관점에서 본 프로젝트의 기본 학습법은 다음과 같이 정리된다.

* **완전 비지도 및 준지도(실험 1–3)**:
  이미지 1장(에피소드)마다 전역 보상 \(R(x)\) 을 정의하고,
  이를 한 스텝 뒤 시점에서 이전 배치의 이벤트들에 대한 리턴으로 사용하는
  **이미지별 전역 보상 on-policy Monte Carlo Actor–Critic** 구조를 따른다.
* **완전지도 gradient mimicry(실험 4)**:
  각 시냅스 이벤트마다 Teacher gradient 와의 차이
  \(R_e^{\text{sup}} = -\lVert \Delta w_i^{\text{agent}} - \Delta w_i^{\text{teacher}} \rVert^2\) 를
  **즉시 보상**으로 사용하는 **Contextual Bandit + PPO** 구조를 따른다.

네 시나리오 모두 REINFORCE with baseline 아이디어를 기반으로 한
**클리핑된 정책경사(PPO)** 를 공통으로 사용한다.

### 1.2 실험 시나리오 인덱스

섹션 번호와 실험 시나리오 번호의 대응은 다음과 같다.

* 3장: 실험 1 (시나리오 1.1, 완전 비지도 단일 정책)
* 4장: 실험 2 (시나리오 1.2, 완전 비지도 두 정책)
* 5장: 실험 3 (시나리오 2, 준지도 단일 정책 분류)
* 6장: 실험 4 (시나리오 3, 완전지도 gradient mimicry)

공통 구성 요소는 2장에서 정리한다.

### 1.3 RL 인과성 문제와 가중치 공간 MDP 관점

#### 1.3.1 기존 구조의 인과성 결여 문제

가장 직관적인(그러나 잘못된) 설계는 다음과 같다.

1. 현재 가중치 (W_k) 로 이미지/에피소드 배치 (k) 를 순전파한다.
2. 이때의 스파이크 궤적만 기록해 두고, 가중치는 **변경하지 않는다**.
3. 에피소드가 끝난 뒤, 기록된 로컬 상태 (s_e) 들을 Actor 에 통과시켜 액션 (a_e) 를 계산한다.
4. 같은 에피소드에서 측정된 전역 보상 (R_k) 를 모든 이벤트에 할당하여 PPO 업데이트를 수행한다.
5. 업데이트된 정책으로부터 나온 액션들을 한꺼번에 적용해 (W_{k+1}) 을 만든다.

이 구조에서는

* 보상 (R_k) 는 **“업데이트 전 가중치 (W_k)”** 로 만들어진 스파이크 궤적에서 계산된다.
* PPO 학습에 쓰이는 액션 (a_e) 는 **에피소드가 끝난 뒤 사후적으로** 계산되며,
  실제로는 (R_k) 생성에 아무 영향도 주지 않았다.

따라서

> “보상 (R_k) 가 액션 (a_e) 의 결과가 아니다”

라는 문제가 생긴다. 즉, 강화학습의 기본 구조인

[
\text{State} \rightarrow \text{Action} \rightarrow \text{Reward}
]

의 인과적 고리가 끊겨 있으며, 이 상태에서 정책경사 (\nabla_\theta \log \pi_\theta(a_e\mid s_e) R_k) 를 계산하면 **이론적으로 기대 그래디언트가 0** 이 되어버린다. (코드 상에서는 잡음과 구현 디테일 때문에 완전히 0 이 되지는 않지만, 의미 있는 policy gradient 로 보기 어렵다.)

이 문제는 완전 비지도/준지도(실험 1–3)에서 특히 치명적이다. 이 시나리오들의 보상은 SNN이 입력을 어떻게 처리했는지(희소성, 다양성, 분류 정확도 등)에 따라 결정되는데, 그 처리과정은 **Actor가 제안한 업데이트를 적용하기 전에 이미 고정된 가중치**로 수행되기 때문이다.

#### 1.3.2 해결책: 가중치 공간에서의 MDP 재정의

이 문제를 해결하기 위해, 본 프로젝트에서는 “스파이크 이벤트 시점”이 아닌 **“가중치 업데이트 시점”** 을 기준으로 MDP 를 재정의한다. 관점을 다음과 같이 바꾼다.

* 에이전트 = “시냅스들을 어떻게 업데이트할지 결정하는 로컬 정책 전체”
* 환경 = “업데이트된 가중치로 SNN 을 돌렸을 때 성능을 알려주는 시스템(데이터셋 + 시뮬레이터)”

즉, **가중치 공간(weight space)** 에서의 MDP 를 정의한다.

* 상태 (S_k): 현재 네트워크 가중치 (W_k) 및 3.3절에서 정의하는 전역 보상 계산에 필요한 환경 요약 통계 전체
  (예: winner 분포 (p^{(K)}), 이미지별 이전 winner 기록 (j_{\text{prev}}(x)) 등으로,
  (R_{\text{sparse}}, R_{\text{div}}, R_{\text{stab}}) 를 계산할 때 사용하는 값들).
  구현에서는 이들 통계를 별도의 버퍼나 모듈에서 관리하지만,
  강화학습 관점에서는 모두 상태 (S_k) 의 일부로 포함된다고 본다.
* 행동 (A_k): 한 에피소드/배치 동안 로컬 정책이 내놓은 액션들을 합친 **전체 가중치 업데이트 (\Delta W_k)**
* 전이:
  [
  W_{k+1} = W_k + \Delta W_k
  ]
* 보상 (R_{k+1}): 업데이트된 가중치 (W_{k+1}) 로 다음 배치(또는 같은 분포에서 샘플한 데이터)를 SNN 에 통과시켰을 때 얻은 전역 보상

이제 인과 구조는

[
(W_k \xrightarrow{A_k=\Delta W_k} W_{k+1}) \xrightarrow{\text{rollout}} R_{k+1}
]

로 정리된다. 즉

> “시점 (k) 에서의 업데이트 (A_k) 가 다음 시점 (k+1) 의 보상 (R_{k+1}) 을 만든다”

라는 점이 명확해지며, **강화학습의 정의를 만족하는 MDP** 로 볼 수 있다.

실제 구현에서는 다음과 같은 외부 루프를 사용한다(완전 비지도/준지도 공통).

1. **Rollout 단계 (성능 평가)**

   * 현재 가중치 (W_k) 로 이미지/에피소드 배치 (k) 를 순전파한다.
   * 이때는 이전 스텝에서 이미 적용된 업데이트까지 포함된 상태이다.
   * 이 배치에서 시나리오별 전역 보상(희소성, 다양성, 분류 정확도+마진 등)을 계산하여 (R_k) 를 얻는다.
2. **PPO 업데이트 단계 (과거 액션 평가)**

   * 직전 배치 (k-1) 에서 수집한 이벤트/상태/액션 ((s_e^{(k-1)}, a_e^{(k-1)})) 에 대해,
     이번 배치에서 얻은 보상 (R_k) 를 리턴으로 사용하여 PPO 업데이트를 수행한다.
     [
     G_e^{(k-1)} = R_k
     ]
   * 이는 “이전에 했던 업데이트 (A_{k-1}) 덕분에 이번 성능 (R_k) 가 나왔다”는 식으로 **한 스텝 밀어서** 신뢰를 할당하는 구조이다.
3. **새 액션 샘플링 및 적용 단계 (가중치 갱신)**

   * 현재 배치 (k) 에서 관측된 로컬 상태 (s_e^{(k)}) 들에 대해 Actor 를 호출하여 액션 (a_e^{(k)}) 를 샘플한다.
   * 이들을 합쳐 전체 업데이트 (\Delta W_k) 를 구성하고,
     [
     W_{k+1} = W_k + \Delta W_k
     ]
     로 가중치를 갱신한다.
   * 다음 배치 (k+1) 는 이 (W_{k+1}) 로 순전파된다.

요약하면,

* **완전 비지도/준지도(실험 1–3)** 에서는
  “시점 (k) 의 액션 (A_k) 를 시점 (k+1) 의 전역 보상 (R_{k+1}) 로 평가”하는 **가중치 공간 MDP 기반 RL** 을 사용한다.
* 이때 롤아웃에 사용되는 정책은 항상 당시에 고정된 (\pi_{\theta_{\text{old}}}) 이며,
  해당 롤아웃에서 나온 데이터만을 사용해 PPO 업데이트를 수행하므로 **on-policy MC PPO** 로 분류할 수 있다.

#### 1.3.3 완전지도(gradient mimicry) 시나리오의 RL 정당화

완전지도 gradient mimicry(실험 4)는 위의 구조와 약간 다르다.

* 이 시나리오에서 보상은
  [
  R = -\bigl\lVert \Delta w^{\text{agent}} - \Delta w^{\text{teacher}} \bigr\rVert^{2}
  ]
  같이 **“Agent 업데이트와 Teacher 업데이트의 차이”** 로 정의된다.
* 즉, Actor 가 내놓은 액션 (a_e) 를 이용해 (\Delta w^{\text{agent}}) 를 구성하는 순간,
  이미 Teacher 업데이트 (\Delta w^{\text{teacher}}) 와 비교하여 **즉시 보상** 이 결정된다.
* 보상은 “미래의 성능”이 아니라, “현재 action 이 Teacher 와 얼마나 일치하는지”만으로 정해지므로
  이는 **Contextual Bandit** 문제에 해당한다.

이 경우에는

* 상태 = 로컬 스파이크 히스토리 + 가중치 + 레이어 인덱스 등 컨텍스트
* 액션 = 해당 시냅스의 업데이트 제안
* 보상 = Teacher 업데이트와의 유사도에 기반한 즉시 보상

이 되고, 각 에피소드/배치에서 관측된 ((s_e, a_e, R_e)) 를 가지고 그대로 PPO 를 적용할 수 있다.
액션이 곧바로 보상을 결정하므로, 인과성 문제는 존재하지 않는다.

정리하면,

* **실험 1–3**: 가중치 공간에서의 MDP, “한 스텝 뒤의 전역 보상”으로 인과성을 회복한 on-policy MC PPO
* **실험 4**: Teacher gradient 와의 차이를 즉시 평가하는 Contextual Bandit 형태의 on-policy PPO

로 해석할 수 있으며, 네 시나리오 모두 강화학습의 관점에서 **인과성이 보장된 정책경사 문제**로 정당화된다.

---

## 2. 공통 구성 요소

### 2.1 데이터셋과 입력 인코딩

* 데이터셋은 **MNIST** 를 사용한다.
* 하나의 에피소드는 이미지 1장을 일정 시간 (T) 동안 SNN 에 흘렸을 때 생성되는 모든 뉴런 스파이크와 시냅스 이벤트 시퀀스 전체를 의미한다. 편의상 이를 “이미지 1장 = 에피소드 1개” 라고 부른다.
* 실제 구현에서는 GPU 효율을 위해 여러 이미지를 한 번에 처리하며, 이때 `--batch-size-images` 는 **한 번의 rollout 에서 동시에 시뮬레이션하는 에피소드(이미지)의 개수**를 뜻한다. 각 이미지는 여전히 독립적인 에피소드로 취급되며, 이미지마다 별도의 전역 보상 (R(x)) 가 계산된다.
* 각 시나리오마다 **시뮬레이션 타임스텝 수** (T) 를 별도로 두고 모두 CLI 하이퍼파라미터로 노출한다.

#### 2.1.1 Poisson 인코딩 (기본 모드)

기본 입력 인코딩은 픽셀 값 (x_{p} \in [0,1]) 에 비례하는 발화율을 사용하는 **Poisson 인코딩**이다. 타임스텝 (t = 1,\dots,T), 픽셀 (p) 에 대해 스파이크 (s_{p}(t) \in {0,1}) 를

[
P\bigl(s_{p}(t)=1\bigr) = \lambda_{p} \Delta t
]

로 두고 샘플링한다. 여기서 (\lambda_{p}) 는 픽셀 값에 비례하는 발화율이며, (\Delta t) 는 단위 시간 간격이다.

요약하면 한 에피소드(이미지 1장)에 대해

1. 이미지를 (T) 타임스텝 동안 Poisson 인코더를 통해 입력층으로 주입하고
2. 그 동안 발생하는 모든 시냅스 이벤트를 수집한 뒤
3. 에피소드 전체에 대한 전역 보상 \(R(x_k)\) 을 계산하고
4. (실험 4 제외) 이 보상을 한 스텝 뒤 시점에서
   직전 배치의 이벤트들에 대한 리턴으로 사용한다
   (1.3절, 2.8절에서 정의한 가중치 공간 MDP 구조).

#### 2.1.2 전류 직주입 인코딩 (완전지도 시나리오 전용 선택지)

완전지도 gradient mimic 시나리오(6장)에서는 BPTT 비용과 스파이크 수를 줄이기 위해, 입력층을 **전류 직주입(current injection)** 방식으로 사용할 수 있는 옵션을 추가한다. 이를 위해 CLI 하이퍼파라미터

* `--sup-input-encoding` (\in {\texttt{poisson}, \texttt{direct}})

을 두고,

* `poisson` 선택 시: 위의 Poisson 인코딩을 그대로 사용한다.
* `direct` 선택 시: 각 픽셀 값 (x_{p}) 을 비율 상수 (\kappa) 와 곱한 전류 (I_{p} = \kappa x_{p}) 를 입력층 뉴런에 **(T_{\text{sup}}) 타임스텝 동안 고정된 값으로 주입**한다.

직주입 모드에서는 스파이크 생성이 확률적 Poisson 샘플링이 아니라, LIF 방정식에 의해 결정되는 **결정론적(deterministic)** 동역학으로 바뀐다. BPTT 비용은 시뮬레이션 길이 (T_{\text{sup}}) 에 선형으로 비례하므로, 실제 구현에서는 (T_{\text{sup}}) 를 비교적 짧게(예: 1–5 스텝) 두는 것을 기본값으로 가정하고, 이를 별도의 CLI 인수(예: `--T-sup`)로 제어한다.

다른 세 시나리오(완전 비지도 2개, 준지도 1개)는 항상 Poisson 인코딩을 사용하며, 완전지도 시나리오에서만 `--sup-input-encoding` 으로 입력 인코딩 방식을 선택할 수 있다.

### 2.2 LIF 뉴런과 스파이크 히스토리

모든 SNN 뉴런은 표준 **Leaky Integrate-and-Fire (LIF)** 모델을 따른다. 막전위 (v(t)) 에 대해

[
\tau_{m} \frac{dv(t)}{dt} = -v(t) + R I_{\text{syn}}(t)
]

막전위가 임계값 (v_{\theta}) 를 넘으면 스파이크를 내고 (v) 는 reset 된다 (구현 편의를 위해 hard reset 사용).

구현상의 주요 가정은 다음과 같다.

* 뉴런마다 단 하나의 스파이크 배열만 유지한다. 예: (s_{j}(t) \in {0,1}, t = 1,\dots,T).
* 시냅스 단위 추적 변수는 필요 시 뉴런 스파이크 배열을 참조해 계산하며, 시냅스별 스파이크 배열은 따로 두지 않는다.

각 MNIST 이미지는 독립된 에피소드이므로, **새로운 이미지를 주입하기 전에** 모든 LIF 뉴런과 시냅스의 동역학 상태(막전위 (v), 누적 전류, refractory 상태 등)는 초기값으로 리셋한다. 이렇게 해서 서로 다른 이미지 에피소드의 스파이크 활동과 막전위가 섞이지 않도록 하고, RL 관점에서 에피소드 간 독립성을 유지한다.

### 2.3 로컬 스파이크 히스토리와 CNN 입력

시냅스 (i) 가 뉴런 (j \to k) 를 연결한다고 하자.

* pre 뉴런 (j) 의 스파이크 배열: (s_{j}(t))
* post 뉴런 (k) 의 스파이크 배열: (s_{k}(t))

에피소드 내 시각 (t) 에서 고정 길이 (L) 의 최근 히스토리 구간을 잘라

[
x_{i}^{\text{pre}}(\tau) = s_{j}(t-\tau), \quad \tau = 0,\dots,L-1
]

[
x_{i}^{\text{post}}(\tau) = s_{k}(t-\tau), \quad \tau = 0,\dots,L-1
]

를 만든다.

두 배열을 길이 (L) 의 2채널 1D 시계열로 묶어

[
X_{i}(t) \in {0,1}^{2 \times L}
]

로 정의한다.

이 설계에서는 로컬 상태를 두 개의 고정 길이 스파이크 배열과 소수의 추가 feature(현재 가중치, 레이어 인덱스, 이벤트 타입 등)만으로 구성한다. 누적 발화 횟수나 마지막 스파이크 이후 경과 시간 같은 추가 요약 스칼라는 사용하지 않는다.

CNN 전단의 입력은 항상 이 2채널 스파이크 배열 (X_{i}(t)) 이며, 이 문서에서 말하는 “스파이크 히스토리”는 곧 이 배열을 의미한다. 여기서 히스토리 길이 (L) 은 모든 시냅스·에피소드에 대해 동일한 고정 길이이며, CLI 하이퍼파라미터 `--spike-array-len` 으로 제어한다. (L) 은 시뮬레이션 타임스텝 수 (T) 와는 독립적으로 설정하지만, 실제 구현에서는 (L \le T) 를 가정한다.

### 2.4 1D CNN 전단 구조

모든 가중치 정책(Actor)과 가치함수(Critic)는 **동일한 1D CNN 전단**을 앞단에 둔다.

* 입력: (X_{i}(t) \in {0,1}^{2 \times L})

구조(구현 기준 예시):

1. Conv1d(in_channels=2, out_channels=16, kernel=5, padding=2, stride=1) + ReLU
2. Conv1d(in_channels=16, out_channels=16, kernel=5, padding=2, stride=1) + ReLU
3. 시간 축 global average pooling → 길이 16 feature 벡터

따라서 CNN 전단의 출력은 항상

[
h_{i}(t) \in \mathbb R^{16}
]

이다.

정책과 Critic 에 대해 공통으로 적용되는 가정은 다음과 같다.

* 정책마다, Critic마다 CNN 전단 파라미터를 공유하지 않는다. 구조는 동일하지만 각 Actor·Critic 이 각각 자신의 CNN+MLP 파라미터를 가진다.
* 실험(시나리오) 간에도 파라미터는 공유하지 않고 각 실험 시작 시 모든 Actor·Critic 을 새로 초기화한다.

### 2.5 로컬 상태 벡터와 이벤트 타입 원핫 인코딩

로컬 상태 벡터는 **CNN feature + 추가 feature(스칼라 및 저차원 벡터)** 를 concat 하는 late fusion 구조로 만든다.

시냅스 (i), 시각 (t), 이벤트 (e = (i,t,\text{type})) 에 대해 항상 포함되는 추가 feature는 다음과 같다.

* 현재 시냅스 가중치 (w_{i}(t))
* 이벤트 타입 원핫 벡터 (\mathbf e_{\text{type}}(e) \in {0,1}^{2}), pre/post 구분

이벤트 타입 원핫 벡터는

[
\mathbf e_{\text{type}}(e) =
\begin{cases}
(1,0)^\top & \text{pre 이벤트일 때} \
(0,1)^\top & \text{post 이벤트일 때}
\end{cases}
]

로 정의한다.

완전지도 실험에서는 여기에 레이어 인덱스를 정규화한 스칼라 (l_{\text{norm},i}) 를 추가로 포함한다.

따라서 입력 벡터는 시나리오별로 다음과 같이 정의한다.

* 완전비지도 및 준지도 시나리오:
  [
  z_{i}^{\text{unsup}}(t) = [h_{i}(t) ; w_{i}(t) ; \mathbf e_{\text{type}}(e)]
  ]
  [
  z_{i}^{\text{semi}}(t) = [h_{i}(t) ; w_{i}(t) ; \mathbf e_{\text{type}}(e)]
  ]
* 완전지도 gradient mimicry 시나리오:
  [
  z_{i}^{\text{sup}}(t) = [h_{i}(t) ; w_{i}(t) ; l_{\text{norm},i} ; \mathbf e_{\text{type}}(e)]
  ]

여기서 ([\cdot ; \cdot]) 는 벡터 concat 을 의미한다.

실제 입력 차원 (d) 는 CNN 출력 차원 16에 추가 feature 차원을 더한 값이다. 예를 들어

* 완전비지도·준지도: (h_{i}(t)) 16차원 + (w_{i}(t)) 1차원 + (\mathbf e_{\text{type}}(e)) 2차원 → (d_{\text{unsup}} = d_{\text{semi}} = 19)
* 완전지도: (h_{i}(t)) 16차원 + (w_{i}(t)) 1차원 + (l_{\text{norm},i}) 1차원 + (\mathbf e_{\text{type}}(e)) 2차원 → (d_{\text{sup}} = 20)

pre/post 구분은 항상 (\mathbf e_{\text{type}}(e)) 로 로컬 상태에 반영되며, 별도의 pre 전용·post 전용 정책은 사용하지 않는다.

### 2.6 가중치 정책 네트워크 (Actor, Gaussian)

각 가중치 정책 네트워크는 입력 (z_{i}(t)) 를 받아 **정규화된 스칼라 가중치 변화량** (\Delta d_{i}(t)) 를 출력한다.

MLP head 구조는 모든 정책에서 동일하며 다음과 같다.

1. 입력층: 차원 (d \to 32) + ReLU
2. 은닉층: (32 \to 32) + ReLU
3. 출력층: (32 \to 1) 선형 → Tanh

출력 Tanh 를 통해 평균 (m_{i}(t) \in [-1,1]) 을 얻고, 이를 중심으로 하는 Gaussian 정책을 정의한다.

* 시나리오·정책별로 서로 다른 표준편차 (\sigma_{\text{policy}}) 를 둔다.
  예: (\sigma_{\text{unsup1}}, \sigma_{\text{unsup2}}, \sigma_{\text{semi}}, \sigma_{\text{sup}})
* 모든 (\sigma_{\text{policy}}) 는 CLI 하이퍼파라미터로 제어한다.

액션은

[
\Delta d_{i}(t) \sim \mathcal N\bigl(m_{i}(t), \sigma_{\text{policy}}^{2}\bigr)
]

으로 샘플링한다.

샘플링된 (\Delta d_{i}(t)) 는 바로 가중치에 더하지 않고, 시나리오별 내부 스케일 (s_{\text{scen}}) 과 로컬 학습률 (\eta_{w}) 를 곱해

[
\Delta w_{i}(t) = \eta_{w}, s_{\text{scen}}, \Delta d_{i}(t)
]

로 변환한다.

가중치는 시나리오별로 정의된 범위에서 클리핑한다.

* **실험 1, 2 (완전 비지도, Diehl–Cook E/I SNN)**
  흥분/억제 시냅스 타입에 따라 별도의 범위를 두며,
  ([w_{\min}^{(\text{exc})}, w_{\max}^{(\text{exc})}]) 는 CLI 인수 `--exc-clip-min`, `--exc-clip-max` 로,
  ([w_{\min}^{(\text{inh})}, w_{\max}^{(\text{inh})}]) 는 `--inh-clip-min`, `--inh-clip-max` 로 설정한다.

* **실험 3 (준지도)와 실험 4 (완전지도 gradient mimicry)**
  흥분/억제를 명시적으로 구분하지 않으므로,
  모든 학습 시냅스에 대해 공통 전역 범위 ([\texttt{w-clip-min}, \texttt{w-clip-max}]) 를 사용한다.
  구현에서는 각 업데이트 후
  [
  w_{i} \leftarrow \operatorname{clip}\bigl(w_{i}, \texttt{w-clip-min}, \texttt{w-clip-max}\bigr)
  ]
  를 적용하여, 로컬 정책이 비정상적으로 큰 가중치를 만들지 못하도록 안정성을 보장한다.

### 2.7 가치함수 네트워크 (Critic)

Critic 은 동일한 입력 (z_{i}(t)) 를 받아 해당 로컬 상태에서의 **기대 누적 보상 추정값** (V_{\phi}(z_{i}(t))) 를 출력한다.

MLP head 구조는 Actor 와 동일하되 마지막 층에서 Tanh 를 쓰지 않고 선형 출력만 둔다.

1. 입력층: (d \to 32) + ReLU
2. 은닉층: (32 \to 32) + ReLU
3. 출력층: (32 \to 1) 선형 → (V_{\phi}(z_{i}(t)))

Actor 와 Critic 은 CNN 전단까지 포함하여 **완전히 다른 파라미터 집합**을 사용하며, 각각 별도의 optimizer 로 학습된다.

### 2.8 RL 궤적 구조와 가중치 공간 시계열

이 프로젝트에는 두 개의 시간 축이 존재한다.

1. **내부 시계열**: 한 이미지/에피소드 안에서의 타임스텝 (t = 1,\dots,T)

   * 이 축에서는 스파이크, 시냅스 이벤트, 로컬 상태 (s_e = z_i(t)) 가 정의된다.
2. **외부 시계열**: 가중치 업데이트가 적용되는 배치 인덱스 (k = 0,1,2,\dots)

   * 이 축에서는 “현재 가중치 (W_k)” 와 “업데이트 (\Delta W_k)” 와 “전역 보상 (R_{k+1})” 이 정의된다.

#### 2.8.1 이벤트 수준 궤적

에피소드(이미지) (x) 와 그 안에서의 이벤트 인덱스 (e = 1,\dots,E(x)) 에 대해

* 상태: (s_e) (2.3–2.5 절에서 정의한 로컬 상태)
* 액션: (a_e) (Actor 가 출력한 정규화된 가중치 변화량 (\Delta d_e))
* Critic 출력: (V_e = V_\phi(s_e))

을 정의한다.

각 이벤트는 결국 어떤 외부 시점 (k) 에 속해 있으며, 이 이벤트들이 합쳐져 **전체 업데이트 (\Delta W_k)** 를 구성한다.

#### 2.8.2 외부 시점에서의 MDP (실험 1–3)

완전 비지도/준지도(실험 1–3)에서는 외부 시점 (k) 에 대해 다음과 같은 MDP 를 정의한다.

* 상태 (S_k): 현재 가중치 (W_k) 와, 3.3절에서 정의한 전역 보상 계산에 필요한 환경 요약 통계
  (예: winner 분포 (p^{(K)}), 이미지별 이전 winner 기록 (j_{\text{prev}}(x)) 등)을 모두 포함한 상태로 본다.
  즉, 보상 (R_{k+1}) 를 계산할 때 참조하는 모든 전역 변수는 개념적으로 (S_k) 의 일부이다.
* 행동 (A_k): 배치 (k) 동안 생성된 이벤트들의 액션 ({a_e^{(k)}}) 를 집계한 전체 업데이트 (\Delta W_k)
* 전이:
  [
  W_{k+1} = W_k + \Delta W_k
  ]
* 보상 (R_{k+1}): 업데이트된 가중치 (W_{k+1}) 로 배치 (k+1) 를 순전파했을 때 얻은 전역 보상

PPO 업데이트에서 사용되는 MC 리턴은

* 배치 (k) 에서 관측된 이벤트들의 집합 (\mathcal E^{(k)}) 에 대해
* 다음 시점 보상 (R_{k+1}) 를 모든 이벤트에 동일하게 할당하여
  [
  G_e^{(k)} = R_{k+1}, \quad e \in \mathcal E^{(k)}
  ]
  로 정의한다.

즉, “이전 배치에서 제안한 업데이트 (\Delta W_k) 덕분에 다음 배치 성능 (R_{k+1}) 가 이렇게 나왔다”는 관점으로 신뢰를 할당하는 구조이다.

#### 2.8.3 완전지도 gradient mimicry (실험 4)

완전지도 gradient mimicry(6장)에서는 Teacher gradient 가 존재하므로, 보상은 “현재 액션이 Teacher 와 얼마나 비슷한지” 로 즉시 정의할 수 있다.

* 상태 (s_e^{\text{sup}}): 로컬 스파이크 히스토리 + 가중치 + 정규화된 레이어 인덱스 + 이벤트 타입
* 액션 (a_e^{\text{sup}}): 해당 시냅스의 업데이트 제안
* Teacher 가 제안하는 업데이트 (\Delta w^{\text{teacher}}_i) 를 미리 계산해 둔다.
* 보상:
  [
  R_e^{\text{sup}}
  = -\bigl\lVert \Delta w^{\text{agent}}_i - \Delta w^{\text{teacher}}_i \bigr\rVert^{2}
  ]

이 경우에는 “한 스텝 뒤의 성능”이 아니라, **현재 액션이 Teacher와 얼마나 잘 맞았는지** 만으로 보상이 결정되므로, 외부 시점 (k) 를 사용할 필요 없이 **Contextual Bandit + PPO** 구조로 볼 수 있다.

### 2.9 Actor–Critic 업데이트와 PPO

에피소드 및 외부 시점을 모두 포함하는 전체 이벤트 집합을 (\mathcal E) 라 하자. 각 이벤트 (e \in \mathcal E) 에 대해

* 상태: (s_e)
* 액션: (a_e)
* Critic 출력: (V_e)
* 보상: (r_e)
* 리턴(누적 보상): (G_e)

를 정의한다.

이 프로젝트에서는 모든 시나리오에서 **할인율을 (\gamma = 1)** 로 두고, 시간에 따른 보상 분포를 따로 모델링하지 않는다.
즉, MC 리턴을 “여러 시점 보상의 합”이 아니라 **한 번의 보상으로 근사하는 1-step MC 리턴** 으로 사용한다. 구체적으로

* 완전 비지도/준지도(실험 1–3):
  외부 시점 (k) 에 대해 “다음 배치 보상” (R_{k+1}) 하나를 전체 미래 성능에 대한 MC 리턴 (G_e)의
  근사치로 사용한다. 즉, (G_e) 를 1-step 리턴 (R_{k+1}) 로 두는 구조이다.
* 완전지도(실험 4):
  각 이벤트에서 즉시 정의되는 보상 (R_e^{\text{sup}}) 을 사용한다.

따라서 모든 이벤트에 대해

[
G_e =
\begin{cases}
R_{k+1} & e \in \mathcal E^{(k)} \text{ (실험 1–3)} \
R_e^{\text{sup}} & \text{(실험 4)}
\end{cases}
\quad
A_e = G_e - V_e
]

로 두고 advantage (A_e) 를 정의한다.

#### 2.9.1 PPO Actor 손실

PPO 는 기본적으로 “advantage 로 가중된 log-prob” 를 최대화하는 REINFORCE with baseline 을 기반으로 하되, 한 번의 업데이트에서 정책이 과도하게 바뀌지 않도록 **확률비(ratio)** 를 클리핑하는 방식으로 안정성을 높인다.

에피소드(또는 외부 시점 배치)를 수집할 때 사용한 정책 파라미터를 (\theta_{\text{old}}) 라 두고

[
r_{e}(\theta)
= \frac{\pi_{\theta}(a_{e} \mid s_{e})}
{\pi_{\theta_{\text{old}}}(a_{e} \mid s_{e})}
]

를 정의한다. 클리핑 범위 하이퍼파라미터를 (\epsilon > 0) 이라 할 때, 본 프로젝트에서 사용하는 PPO Actor 손실은

[
L_{\text{actor}}^{\text{PPO}}(\theta)
= -\frac{1}{|\tilde{\mathcal E}|} \sum_{e \in \tilde{\mathcal E}}
\min\Bigl(
r_{e}(\theta) A_{e},
\operatorname{clip}\bigl(r_{e}(\theta), 1-\epsilon, 1+\epsilon\bigr) A_{e}
\Bigr)
]

이다. 여기서 (\tilde{\mathcal E} \subseteq \mathcal E) 는 실제로 업데이트에 사용되는 서브샘플 이벤트 집합(2.9.3절)이다.

* (\theta_{\text{old}}) 는 해당 롤아웃(또는 외부 시점)에서 고정된 정책 파라미터이다.
* 업데이트 동안 (\theta) 는 gradient 에 따라 변하지만, 분모의 (\pi_{\theta_{\text{old}}}) 는 항상 rollout 당시 정책으로 고정된다.
* (r_{e}(\theta)) 가 (1 \pm \epsilon) 범위를 벗어나면, 추가적인 정책 변화가 더 이상 목적함수 증가로 이어지지 않으므로 **한 번의 업데이트에서 정책이 과도하게 바뀌는 것을 제한**하는 효과가 있다.

#### 2.9.2 Critic 손실

Critic 은 MC 리턴 (G_{e}) 를 회귀하는 value function 으로 보고, 전형적인 MSE 손실을 사용한다.

[
L_{\text{critic}}(\phi)
= \frac{1}{|\tilde{\mathcal E}|} \sum_{e \in \tilde{\mathcal E}} \bigl(G_{e} - V_{e}\bigr)^{2}
]

업데이트는

[
\phi \leftarrow \phi - \eta_{\text{critic}} \nabla_{\phi} L_{\text{critic}}
]

로 수행한다. Actor–Critic 구조 전체는 “MC 리턴 기반 advantage Actor–Critic + PPO 클리핑” 으로 해석할 수 있다.

#### 2.9.3 이미지 단위 미니배치와 per-image 이벤트 서브샘플링

VRAM 사용량과 연산 시간을 제어하기 위해, 실제 업데이트에서는 다음과 같은 구조를 사용한다.

1. 이미지 1장당 에피소드 버퍼 1개를 만들고
2. 여러 이미지를 모아 에피소드 버퍼들을 이어 붙여
3. **이미지 단위 미니배치**를 구성한 뒤
4. 이 미니배치에서 얻은 이벤트들 중 일부만 선택해 PPO 업데이트에 사용한다.

여기서 핵심은 **VRAM 사용량과 연산 시간을 제어하기 위해, 이미지마다 고정 개수 (K) 개의 이벤트만 서브샘플링**한다는 점이다. 이를 위해 CLI 하이퍼파라미터

* `--events-per-image` (= K)

를 두고, 실제 구현은 다음과 같이 동작한다.

* 미니배치에 포함된 각 이미지(에피소드) (j) 에 대해, 에피소드 동안 발생하는 이벤트 스트림 (e = 1,\dots,E_{j}) 를 시간 순서대로 훑는다.
* 각 이미지마다 용량이 (K) 인 **저수지(reservoir) 버퍼** (\mathcal E_{j}) 를 하나 두고, 고전적인 **저수지 샘플링(reservoir sampling)** 알고리즘으로 이벤트를 선택한다.

  * (e \le K) 인 동안에는 들어오는 이벤트를 그대로 (\mathcal E_{j}) 에 추가한다.
  * (e > K) 부터는, 확률 (K/e) 로 버퍼 내 임의의 인덱스를 골라 현재 이벤트로 교체한다.
* 에피소드가 끝나면, (\mathcal E_{j}) 는 해당 이미지에서 발생한 이벤트들 중 **균등확률에 가까운 무작위 표본 (\min(K,E_{j})) 개**를 담고 있게 된다.

미니배치 전체에 대해, 선택된 이벤트들의 합집합을

[
\tilde{\mathcal E} = \bigcup_{j=1}^{N} \mathcal E_{j}
]

로 두면, 한 번의 PPO 업데이트에서 실제로 사용하는 이벤트 수는

[
|\tilde{\mathcal E}| \le N K
]

가 된다. 여기서 (N) 은 이미지 미니배치 크기(`--batch-size-images`)이다.

이때 각 이미지 (j) 에 대해, 그 이미지에서 발생한 전체 이벤트 집합을
(\mathcal E^{\text{full}}_{j} = \{1,\dots,E_{j}\}), 저수지 샘플링으로 선택된
서브샘플을 (\mathcal E_{j}) 라고 쓰면, 이 프로젝트가 실제로 최소화하려는
**이미지 단위 목적함수**는 각 이미지별 이벤트 평균을 다시 이미지 평균한

[
\frac{1}{N} \sum_{j=1}^{N}
\frac{1}{|\mathcal E^{\text{full}}_{j}|}
\sum_{e \in \mathcal E^{\text{full}}_{j}} f(e)
]

형태로 볼 수 있다. (여기서 (f(e)) 는 2.9.4절의 per-event PPO 손실 항 전체를 의미하는 약칭이다.)
per-image 저수지 샘플링과 “이미지마다 동일한 (\le K) 개” 라는 제약 때문에,
(\tilde{\mathcal E} = \bigcup_{j=1}^{N} \mathcal E_{j}) 위에서의 단순 이벤트 평균

[
\frac{1}{|\tilde{\mathcal E}|} \sum_{e \in \tilde{\mathcal E}} f(e)
]

은 위 이미지 단위 목적의 좋은 확률적 근사(대부분의 이미지에서 (E_j \ge K) 인 상황에서는
무편향 추정치)가 된다. 즉, 이벤트 수가 많은 이미지가 과도하게 큰 비중을 갖지 않고,
모든 이미지가 거의 동일한 비중으로 PPO 업데이트에 기여하도록 설계되어 있다.

수학적으로 보면, 모든 이벤트 집합 (\mathcal E) 에 대해 정의된 이상적인 PPO 손실

[
\frac{1}{|\mathcal E|} \sum_{e \in \mathcal E} f(e)
]

을, 서브샘플링된 이벤트 집합 (\tilde{\mathcal E}) 에 대한 **Monte Carlo 추정치**

[
\frac{1}{|\tilde{\mathcal E}|} \sum_{e \in \tilde{\mathcal E}} f(e)
]

로 근사하는 것에 해당한다.
저수지 샘플링은 시간 순서대로 들어오는 스트림에 대해 균등표본을 보장하고,
각 이미지마다 동일한 최대 (K) 개의 이벤트만 선택하므로, 2.9.3절에서 정의한
이미지 단위 목적에 대해 full-batch PPO 와 거의 동일한 기대값을 갖는 경사 추정치를 제공한다.
대신 per-update 분산이 다소 커지지만, 이는 더 많은 epoch 와 적절한 learning rate 조정으로 상쇄한다.

실제 구현에서는 (\tilde{\mathcal E}) 를 다시 `--event-batch-size` 크기의 이벤트 미니배치로 나눈 뒤, 각 미니배치에 대해 전체 PPO 손실 (L_{\text{PPO}}(\theta,\phi)) 를 계산하고 `--ppo-epochs` 회만큼 gradient 업데이트를 수행한다.

#### 2.9.4 전체 PPO 손실 요약

이 프로젝트에서의 **RL 학습 과정은 PPO 기반 정책 경사법 한 가지로 통일**하며,
학습 동안 **최적화하는 전체 손실은, per-image 저수지 샘플링을 통해 구현된
이벤트 미니배치 평균(= 이미지 단위 평균의 근사)으로서 다음과 같이 정의**한다.

[
L_{\text{PPO}}(\theta,\phi)
= \mathbb{E}*{e \in \tilde{\mathcal E}}\Bigl[
L*{e}^{\text{CLIP}}(\theta)
+ c_{v}\bigl(G_{e}-V_{\phi}(s_{e})\bigr)^{2}
\Bigr].
]

여기서

* (\tilde{\mathcal E}) 는 2.9.3절에서 정의한 **per-image 저수지 샘플링으로 선택된 이벤트 집합**이다.
* (L_{e}^{\text{CLIP}}(\theta)) 는 2.9.1절에서 정의한 per-event PPO 클리핑 Actor 손실이다.
* (G_{e}) 는 MC 리턴이며, 본 설계에서는 외부 시점/시나리오에 따라
  “다음 배치에서 측정한 전역 보상” 또는 “Teacher 기반 즉시 보상”으로 정의된다.
* (V_{\phi}(s_{e})) 는 상태 (s_{e}) 에 대한 Critic 출력이다.
* (c_{v}>0) 는 value 손실(제곱 오차)의 비중을 조절하는 하이퍼파라미터다.

요약하면, 이 프로젝트의 RL 업데이트는 항상

* **이미지 단위 on-policy MC PPO**
* **per-image 저수지 샘플링 기반 이벤트 서브샘플링**
* **이벤트 미니배치 SGD**
* (실험 1–3) **가중치 공간 MDP 기반 한 스텝 뒤 보상 사용**
* (실험 4) **Teacher 기반 Contextual Bandit 보상 사용**

의 네 요소를 결합한 형태이며,
이 공통 틀 위에 3–6장에서 각 시나리오별 SNN 구조와 전역 보상 정의만 달리 얹는 구조로 전체 실험을 설계한다.

---

## 3. 실험 1 (시나리오 1.1): 완전 비지도 단일 가중치 정책

### 3.1 목표

실험 1의 목표는 다음과 같다.

* Diehl–Cook 스타일 E/I SNN 에서
* **단 하나의 로컬 가중치 정책 (\pi_{\text{single}})** 만으로

다음과 같은 현상이 어느 정도까지 자연스럽게 형성되는지 관찰한다.

* 흥분/억제 패턴(양수/음수 가중치 구조)
* 뉴런 역할 분화(특정 뉴런이 특정 숫자에 특화되는지 여부)
* 과도하게 지배적인 뉴런 없이 여러 뉴런이 역할을 나누는지 여부

완전 비지도 학습이므로 학습 시 라벨을 사용하지 않고, 평가 시에만 Diehl & Cook(2015) 에서 사용한 **뉴런 라벨링(neuron labeling)** 으로 분류 정확도를 측정한다.

### 3.2 SNN 구조 (Diehl–Cook 아키텍처)

기본 구조는 Diehl & Cook(2015)의 주요 아이디어를 따른다.

* 입력층: MNIST 픽셀 784개
* 흥분층(E): (N_{E}) 개 LIF 뉴런
* 억제층(I): (N_{I} = N_{E}) 개 LIF 뉴런

연결 구조는 다음과 같다.

1. 입력층 → 흥분층(Input→E): 가변 가중치, 흥분성 시냅스
2. 흥분층 → 억제층(E→I): 각 흥분 뉴런 (e_{j}) 와 억제 뉴런 (i_{j}) 를 1:1로 묶는 고정 회로
3. 억제층 → 흥분층(I→E): 모든 억제 뉴런에서 2의 과정으로 인해 1:1 연결된 흥분뉴런을 제외한 모든 흥분 뉴런으로 연결, 가변 가중치, 억제성 시냅스

흥분/억제 시냅스의 부호 및 범위는

* 흥분 자리 가중치: ([\text{exc-clip-min}, \text{exc-clip-max}])
* 억제 자리 가중치: ([\text{inh-clip-min}, \text{inh-clip-max}])

로 클리핑하며, 두 범위는 공통 CLI 하이퍼파라미터로 제어한다.

### 3.3 비지도 전역 보상: 희소성·다양성·안정성

전역 보상은 세 부분으로 구성된다.

* 목표 발화율 보상 (R_{\text{sparse}}): 평균 발화율이 목표값 (\rho_{\text{target}}) 에 가깝도록 유도하는 항
* 다양성 보상 (R_{\text{div}}): 여러 에피소드에 걸쳐 winner 가 골고루 분포되도록 하는 항
* 안정성 보상 (R_{\text{stab}}): 같은 이미지를 반복해서 보여줄 때 winner 뉴런이 매번 뒤집히지 않도록 하는 항

정확한 수식은 이전 버전과 동일하게 유지한다.

[
R_{\text{sparse}}
= -\bigl(\bar r - \rho_{\text{target}}\bigr)^{2}
]

[
R_{\text{div}}^{(K)}
= - \sum_{j=1}^{N_{E}}
\bigl(p_{j}^{(K)} - \tfrac{1}{N_{E}}\bigr)^{2}
]

[
R_{\text{stab}}(x_{n}) =
\begin{cases}
0 & \text{첫 방문} \
+1 & j_{\text{curr}}(x_{n}) = j_{\text{prev}}(x_{n}) \
-1 & j_{\text{curr}}(x_{n}) \neq j_{\text{prev}}(x_{n})
\end{cases}
]

최종 전역 보상은

[
R = \alpha_{\text{sparse}} R_{\text{sparse}} + \alpha_{\text{div}} R_{\text{div}}^{(K)} + \alpha_{\text{stab}} R_{\text{stab}}(x_{n})
]

으로 정의한다.

### 3.4 RL 학습 절차 (가중치 공간 MDP 관점)

실험 1에서의 외부 루프는 1.3과 2.8에서 정의한 가중치 공간 MDP 구조를 그대로 따른다.

* 외부 시점 (k) 마다

  1. **Rollout**: 현재 가중치 (W_k) 로 이미지 배치 (k) 를 Poisson 인코딩 후 시뮬레이션하고,
     각 이미지에 대해 (R_{\text{sparse}}, R_{\text{div}}, R_{\text{stab}}) 을 합쳐 전역 보상 (R_k(x)) 를 계산한다.
  2. **PPO 업데이트**: 직전 배치 (k-1) 에서 수집한 이벤트/액션에 대해,
     “이번” 배치에서의 보상 (R_k) (필요 시 평균 또는 합으로 집계)를 MC 리턴 (G_e^{(k-1)}) 으로 사용하여 PPO 업데이트를 수행한다.
  3. **액션 샘플링 및 적용**: 현재 배치 (k) 에서 관측된 로컬 상태들에 대해 Actor 를 호출해 액션들을 샘플하고, 이를 집계하여 (\Delta W_k) 를 구성한 뒤 (W_{k+1} = W_k + \Delta W_k) 로 갱신한다.

이때

* **이벤트 수준 상태/액션**은 여전히 “단일 시냅스 수준”의 로컬 정보를 본다.
* 다만 **신뢰 할당(credit assignment)** 는 “한 배치 뒤 보상” 수준에서 이루어지며,
  이는 연산량 제약을 고려한 공학적 타협이자, 가중치 공간 MDP 관점에서 이론적으로 정당한 방식이다.

### 3.5 뉴런 라벨링 기반 평가 및 로그·산출물

완전 비지도 실험에서는 학습 단계에서 라벨을 사용하지 않지만 **Diehl–Cook 스타일 뉴런 라벨링**을 통해 분류 정확도를 정의하고 평가한다.

뉴런 라벨링 절차는 다음과 같다.

1. 학습이 충분히 진행된 뒤 training set 전체를 SNN 에 통과시키면서 각 이미지에 대해 정답 라벨 $y$ 와 흥분층 뉴런 $j$ 의 스파이크 수(또는 발화율) $r_{j}(x)$ 를 기록한다.
2. 각 뉴런 $j$ 와 각 클래스 $y$ 에 대해 해당 클래스에 속하는 모든 예제에 대한 $r_{j}(x)$ 를 평균 내어 $\bar r_{j,y}$ 를 계산한다.
3. 뉴런 $j$ 의 라벨을
   $$
   L_{j} = \arg\max_{y} \bar r_{j,y}
   $$
   로 정의한다.
4. 어떤 입력 $x$ 에 대해 예측할 때는

   * 흥분층 뉴런들의 반응 $r_{j}(x)$ 를 모으고
   * 각 클래스 $c$ 에 대해 $L_{j} = c$ 인 뉴런 집합 $\mathcal N_{c}$ 를 구성한다.
   * 클래스 점수:
     $$
     S_{c}(x) = \frac{1}{\lvert \mathcal N_{c} \rvert} \sum_{j \in \mathcal N_{c}} r_{j}(x)
     $$
   * 최종 예측 라벨:
     $$
     \hat y = \arg\max_{c} S_{c}(x)
     $$

이 라벨링을 사용해 train / validation / test accuracy 를 계산하고, 특히 **RL 모델의 train accuracy 변화 곡선**을 기록한다.

실험 1 종료 후 최소한 다음 산출물을 저장한다.

1. $\Delta t$–$\Delta d$ 산점도(STDP 형태 분석): pre–post 시간차 $\Delta t$ 와 정책 액션 $\Delta d$ 를 모아 산점도 형태로 저장한다.
2. 학습 진행 곡선: 에피소드 수(또는 사용한 이미지 수)에 따른 $R_{\text{sparse}}, R_{\text{div}}, R_{\text{stab}}$ 및 뉴런 라벨링 기반 train / validation / test accuracy 의 변화를 그래프로 기록한다.
3. 텍스트 로그: 실험 시작/종료 시각, 최종 평가 지표, 사용한 모든 하이퍼파라미터 값을 담은 텍스트 로그 파일을 남긴다. JSON 은 사용하지 않는다.

---

## 4. 실험 2 (시나리오 1.2): 완전 비지도 두 가중치 정책

### 4.1 목표

실험 2의 목표는 다음과 같다.

* 흥분성 시냅스와 억제성 시냅스를 서로 다른 정책으로 분리했을 때
* 동일한 비지도 보상 하에서

  * 두 정책이 어떤 역할을 학습하는지
  * 흥분/억제 가중치 분포와 기능 분화 양상이 어떻게 달라지는지

를 관찰한다.

학습 중에는 라벨을 사용하지 않고, 평가 시에는 실험 1과 동일한 뉴런 라벨링 기반 분류 정확도를 사용한다.

### 4.2 SNN 구조와 정책 배치

SNN 구조는 3장의 Diehl–Cook 아키텍처와 동일하다.

* Input→E: 흥분성 시냅스
* I→E: 억제성 시냅스
* E→I: 1:1 고정 회로

정책은 다음과 같이 구분한다.

* (\pi_{\text{exc}}): Input→E 시냅스에 적용되는 정책
* (\pi_{\text{inh}}): I→E 시냅스에 적용되는 정책

두 정책은 구조는 같되 CNN+MLP 파라미터는 서로 독립이다.

가중치 클리핑 범위는 실험 1과 동일하게

* 흥분 자리: ([\text{exc-clip-min}, \text{exc-clip-max}])
* 억제 자리: ([\text{inh-clip-min}, \text{inh-clip-max}])

### 4.3 보상과 학습 절차

전역 보상 (R) 의 정의는 3.3과 동일하며, RL 학습 절차는 3.4에서 정의한 가중치 공간 MDP 기반 on-policy MC PPO 를 그대로 따른다. 차이는 다음 한 가지뿐이다.

* 이벤트가 Input→E 시냅스에 속하면 (\pi_{\text{exc}})
* 이벤트가 I→E 시냅스에 속하면 (\pi_{\text{inh}})

의 log-prob 를 사용한다는 점.

Critic 구조와 업데이트, per-image 이벤트 서브샘플링, 이벤트 미니배치 SGD 구조는 실험 1과 동일하다.

### 4.4 로그 및 산출물

실험 2 종료 후에는 다음을 저장한다.

1. 정책별 $\Delta t$–$\Delta d$ 산점도: $\pi_{\text{exc}}, \pi_{\text{inh}}$ 각각에 대해 pre–post 시간차 $\Delta t$ 와 액션 $\Delta d$ 의 관계를 나타내는 산점도를 따로 저장한다.
2. 정책별 가중치 분포: 학습 전/후의 흥분성·억제성 가중치 히스토그램을 정책별로 비교한다.
3. 학습 진행 곡선 및 텍스트 로그: 비지도 보상 항들의 변화, 뉴런 라벨링 기반 train / validation / test accuracy, 특히 RL 모델의 train accuracy 변화 곡선을 그래프로 기록하고, 모든 메타데이터와 하이퍼파라미터 및 최종 지표를 텍스트 로그로 저장한다. JSON 은 사용하지 않는다.

---

## 5. 실험 3 (시나리오 2): 준지도 단일 정책 분류

### 5.1 목표

실험 3의 목표는 다음과 같다.

* 앞선 비지도 실험에서 사용한 **동일한 로컬 정책 프레임워크**를 유지하면서
* SNN 자체가 **직접 분류기를 겸하는 구조**에서
* 분류 정확도와 마진을 전역 보상으로 사용했을 때

어느 정도 수준의 성능과 표현력을 얻을 수 있는지 분석한다.

### 5.2 SNN 구조

준지도 실험에서는 구조를 단순화한다.

* 입력층: 784
* 히든 LIF 층: (N_{\text{hidden}}) (CLI 하이퍼파라미터)
* 출력층: 10 LIF 뉴런

학습 대상 가중치는 Input→Hidden, Hidden→Output 전체이다. 모든 학습 시냅스는 단일 정책 (\pi_{\text{semi}}) 를 공유한다.

이 시나리오에서는 흥분/억제 타입을 명시적으로 구분하지 않으므로,
모든 학습 시냅스에 대해 전역 가중치 클리핑 범위 ([\texttt{w-clip-min}, \texttt{w-clip-max}]) 를 적용한다.
구현에서는 각 업데이트 후

[
w_{i} \leftarrow \operatorname{clip}\bigl(w_{i}, \texttt{w-clip-min}, \texttt{w-clip-max}\bigr)
]

를 수행한다.

출력층 뉴런 인덱스와 라벨은 1:1로 대응된다. 예를 들어 출력층 뉴런 (k) 는 숫자 (k) 를 의미한다.

### 5.3 순전파와 출력 해석

에피소드 하나는 입력–라벨 쌍 ((x,y)) 를 SNN 에 (T_{\text{semi}}) 스텝 동안 주입했을 때
생성되는 모든 스파이크 및 시냅스 이벤트 시퀀스 전체에 대응한다.

1. 입력 (x) 를 Poisson 인코딩해 (T_{\text{semi}}) 스텝 동안 SNN 에 주입한다.
2. 시뮬레이션 동안 이벤트마다 로컬 상태 (s_e) 를 기록한다.
3. 에피소드 종료 후 출력층 뉴런 (k) 에 대해 스파이크 수 (s_{k}) 와 발화율 (r_{k} = s_{k} / T_{\text{semi}}) 를 계산한다.
4. 예측 라벨은
   [
   \hat y = \arg\max_{k} r_{k}
   ]
   로 정의한다.

### 5.4 보상 설계와 마진 개념

분류 문제를 위해 전역 보상 (R) 을 다음 두 항의 합으로 설계한다.

1. 기본 분류 보상 (R_{\text{cls}}):

   * (\hat y = y) 이면 (R_{\text{cls}} = 1)
   * (\hat y \neq y) 이면 (R_{\text{cls}} = -1)

2. 마진 기반 보상 (R_{\text{margin}}):

   * 정답 뉴런의 발화율을 (r_{y}), 오답 중에서 가장 많이 쏜 뉴런의 발화율을
     (r_{\text{max,wrong}}) 라 두고
     [
     M = r_{y} - r_{\text{max,wrong}}
     ]
     를 **마진(margin)** 으로 정의한다.
   * (R_{\text{margin}} = \beta M) ((\beta) 는 스케일 하이퍼파라미터)

최종 전역 보상은

[
R = R_{\text{cls}} + R_{\text{margin}}
]

이다.

마진 항을 포함하는 이유는

* “조금 맞춘 샘플”과 “아주 확신 있게 맞춘 샘플”을 구분하기 위해
* 틀린 샘플에 대해서도 “얼마나 나쁘게 틀렸는지” 를 등급화하기 위해
* 결과적으로 결정 경계와 표현력을 개선하기 위해

이다.

### 5.5 RL 학습 절차

실험 3의 RL 학습 절차 역시 1.3과 2.8에서 정의한 **가중치 공간 MDP 기반 on-policy MC PPO** 를 그대로 따른다.

* 외부 시점 (k) 마다

  * 현재 가중치 (W_k) 로 분류 배치 (k) 를 시뮬레이션하여, 각 샘플에 대해 (R_{\text{cls}} + R_{\text{margin}}) 을 계산하고 이를 집계하여 (R_k) 를 얻는다.
  * 직전 배치 (k-1) 에서 수집된 이벤트/액션에 대해 (G_e^{(k-1)} = R_k) 를 사용해 PPO 업데이트를 수행한다.
  * 현재 배치 (k) 의 이벤트/상태에 대해 Actor 를 호출하여 (\Delta W_k) 를 구성하고, 이를 적용해 (W_{k+1}) 을 만든다.

이 구조 덕분에, 스파이크마다 Actor 를 호출하지 않고도 **분류 성능을 기준으로 한 on-policy RL** 을 구현할 수 있다.

### 5.6 산출물

실험 3 종료 후에는 다음을 저장한다.

1. 분류 정확도 곡선: train / validation / test accuracy 를 에피소드 수 또는 epoch 에 따라 플롯한다. RL 모델의 train accuracy 변화 곡선을 명시적으로 저장한다.
2. 마진 분포: 마진 $M = r_{y} - r_{\text{max,wrong}}$ 의 히스토그램을 학습 전·후로 비교해 출력층 뉴런의 분리 정도를 시각화한다.
3. $\Delta t$–$\Delta d$ 산점도 및 가중치 분포: pre–post 시간차와 액션 $\Delta d$ 의 관계를 나타내는 산점도와, 학습 전/후 가중치 히스토그램을 저장한다.
4. 텍스트 로그: 전체 실험 설정, 최종 분류 성능, 하이퍼파라미터를 텍스트 로그 파일에 기록한다. JSON 은 사용하지 않는다.

---

## 6. 실험 4 (시나리오 3): 완전지도 gradient mimicry

### 6.1 목표

실험 4의 핵심 질문은 다음과 같다.

> 로컬 가중치 정책이 **전역 supervised gradient 정보를 얼마나 잘 활용해서** 유의미한 업데이트를 만들어 내는가?

이를 위해

* surrogate gradient + BPTT 로 얻은 (\partial \mathcal L_{\text{sup}} / \partial w_{i}) 를 Teacher gradient 로 보고
* 로컬 정책이 만들어 낸 실제 가중치 변화 (\Delta w_{i}^{\text{agent}}) 와
* Teacher 가 제안하는 기준 업데이트 (\Delta w_{i}^{\text{teacher}})

의 거리(방향+크기)를 단일 보상 지표로 사용하는 실험을 설계한다.

또한 같은 구조의 SNN 을

* Teacher gradient 를 이용해 직접 supervised 학습시킨 기준선 모델
* 로컬 RL 정책으로 학습시킨 모델

로 나누어 분류 정확도와 loss 수렴 속도를 비교한다.

### 6.2 SNN 구조

완전지도 실험에서는 다음과 같은 SNN 을 사용한다.

* 입력층: 784
* 은닉층: 256
* 은닉층: 128
* 은닉층: 64
* 은닉층: 32
* 출력층: 10 LIF 뉴런

입력을 제외한 모든 층 사이의 시냅스가 학습 대상이다. 모든 학습 시냅스는 단일 정책 (\pi_{\text{grad}}) 의 제어를 받으며,
가중치 값은 전역 클리핑 범위 ([\texttt{w-clip-min}, \texttt{w-clip-max}]) 안에 유지한다.
구현에서는 각 업데이트 후

[
w_{i} \leftarrow \operatorname{clip}\bigl(w_{i}, \texttt{w-clip-min}, \texttt{w-clip-max}\bigr)
]

를 적용하여, Teacher gradient 기반 보상이 과도하게 큰 업데이트를 유도하더라도 실제 시냅스 가중치는 안전한 범위 내에 머무르도록 한다.

각 시냅스에는 정규화된 레이어 인덱스 (l_{\text{norm},i}) 를 부여한다 (예: 입력 직후 히든층 0.2, 그 다음 0.4, 마지막 히든층 0.8, 출력층 1.0 등).

### 6.3 로컬 상태와 Teacher gradient

완전지도 실험에서 로컬 상태는 2.5의 정의에 레이어 인덱스를 추가한 형태이다.

[
z_{i}^{\text{sup}}(t) = [h_{i}(t) ; w_{i}(t) ; l_{\text{norm},i} ; \mathbf e_{\text{type}}(e)]
]

Teacher gradient 는 surrogate gradient + BPTT 로 얻은 (\partial \mathcal L_{\text{sup}} / \partial w_{i}) 를 적절히 스케일링하여

[
\Delta w_{i}^{\text{teacher}} = -\eta_{\text{sup}} \frac{\partial \mathcal L_{\text{sup}}}{\partial w_{i}}
]

로 정의한다.

### 6.4 보상 설계와 RL 해석

이 시나리오에서의 핵심 아이디어는

> “로컬 정책이 Teacher 업데이트를 얼마나 잘 모사하는지를 강화학습의 보상으로 삼는다”

는 것이다.

시냅스 (i) 에 대해

* Actor 가 제안한 업데이트: (\Delta w_{i}^{\text{agent}})
* Teacher 업데이트: (\Delta w_{i}^{\text{teacher}})

라 하면, per-synapse 보상은

[
R_{i}
= -\bigl\lVert \Delta w_{i}^{\text{agent}} - \Delta w_{i}^{\text{teacher}} \bigr\rVert^{2}
]

로 정의할 수 있다. 구현에서는 안정성을 위해 적절한 스케일링과 클리핑을 추가할 수 있다.

이 보상은 **현재 액션이 Teacher 와 얼마나 일치하는지** 만으로 결정되며, “미래의 성능”이나 “다음 배치 결과”를 기다릴 필요가 없다. 따라서

* 상태 = 로컬 스파이크 히스토리 + 현재 가중치 + 레이어 인덱스 + 이벤트 타입
* 액션 = 해당 시냅스의 업데이트 제안
* 보상 = Teacher 업데이트와의 L2 손실에 음수 부호를 붙인 값

이라는 **Contextual Bandit** 문제로 해석할 수 있고, 여기에도 PPO 를 그대로 적용할 수 있다.

### 6.5 on-policy RL로서의 정당화

완전지도 gradient mimicry 시나리오는 다음 의미에서 **on-policy RL/PPO** 로 정당화된다.

1. **인과성**

   * 액션 (\Delta w_{i}^{\text{agent}}) 가 결정되는 즉시, Teacher 업데이트 (\Delta w_{i}^{\text{teacher}}) 와의 차이로부터 보상 (R_i) 가 결정된다.
   * 보상은 액션에 대한 **직접적인 함수** 이며, 그 반대 방향의 의존성은 없다.

2. **on-policy 조건**

   * 데이터를 생성할 때 사용한 정책 (\pi_{\theta_{\text{old}}}) 로부터 나온 상태–액션–보상 ((s_e, a_e, R_e)) 만을 사용하여 PPO 업데이트를 수행한다.
   * replay buffer 나 과거 정책에서 나온 데이터를 재사용하지 않으므로, 전형적인 on-policy PPO 구조에 해당한다.

3. **PPO 구조**

   * Actor 손실은 2.9.1의 클리핑된 정책경사 (L_{\text{actor}}^{\text{PPO}}) 를 그대로 사용한다.
   * Critic 은 Teacher 기반 보상/리턴 (G_e = R_e) 를 회귀한다.
   * 이로써, Teacher gradient 를 직접 사용하는 supervised baseline 과, Teacher 를 **정책학습의 reward signal**로 사용하는 RL 구조를 비교할 수 있다.

### 6.6 산출물

실험 4 종료 후에는 다음을 기록한다.

1. gradient 정렬 지표 곡선: $-\bigl(\Delta w_{i}^{\text{agent}} + \eta_{\text{align}} g_{i}\bigr)^{2}$ 의 평균 및 분포를 에피소드 수에 따라 기록한다.
2. 분류 성능 비교 곡선: 동일 구조를 Teacher gradient 로 직접 supervised 학습시킨 기준선 SNN 과 로컬 RL 정책으로 학습한 SNN 에 대해 train / validation / test accuracy 와 supervised loss 를 epoch 또는 에피소드 수에 따라 비교한다. 특히 RL 모델의 train accuracy 변화 곡선을 명시적으로 저장한다.
3. $\Delta t$–$\Delta d$ 산점도 및 기타 통계: 로컬 정책이 만들어낸 업데이트 패턴을 분석하기 위해 pre–post 시간차 $\Delta t$ 와 액션 $\Delta d$ 의 관계를 나타내는 산점도를 저장한다.
4. 텍스트 로그: 실험 시간, 최종 분류 성능, 모든 하이퍼파라미터 등을 하나의 텍스트 로그 파일로 남긴다. JSON 형식은 사용하지 않는다.

## 7. 기대 기여와 해석

네 가지 실험을 통해 기대하는 기여와 해석 포인트는 다음과 같다.

1. 로컬 가중치 정책의 표현력 평가  
   단일 정책, 두 정책, 준지도 분류, gradient mimicry 등 다양한 설정에서 로컬 정책이 기능 분화, 흥분/억제 패턴 형성, 전역 supervised gradient 정보 활용을 얼마나 책임질 수 있는지 정량·정성적으로 평가한다.

2. 신뢰할당 문제와 gradient mimicry 의 역할  
   비지도·준지도 시나리오에서는 에피소드 단위 전역 보상만 존재하여 전형적인 신뢰할당 문제가 나타난다. gradient mimicry 시나리오에서는 Teacher gradient 를 통해 좋은 업데이트 방향과 크기에 대한 정보를 직접 제공받아 신뢰할당 문제를 어느 정도 완화할 수 있다.

3. 마진과 뉴런 역할 분화  
   준지도 실험에서 마진 기반 보상을 사용할 경우, 정답 뉴런과 오답 뉴런의 발화율 차이가 어떻게 변하는지, 출력층 뉴런이 라벨별 전용 뉴런으로 분화되는지 관찰한다.


## 8. 공통 CLI 하이퍼파라미터 정리

문서 전반에서 등장하는 주요 하이퍼파라미터들을 CLI 인수 관점에서 한 번에 정리하면 다음과 같다.

### 8.1 시뮬레이션 관련

* `--T-unsup1`: 시나리오 1.1 의 타임스텝 $T_{\text{unsup1}}$
* `--T-unsup2`: 시나리오 1.2 의 타임스텝 $T_{\text{unsup2}}$
* `--T-semi`: 시나리오 2 의 타임스텝 $T_{\text{semi}}$
* `--T-sup`: 시나리오 3 의 타임스텝 $T_{\text{sup}}$
* `--dt`: 시뮬레이션 시간 간격(필요 시)

### 8.2 SNN 구조 관련

* `--spike-array-len`: 로컬 스파이크 히스토리 길이 $L$ (2.3절의 $X_{i}(t) \in \{0,1\}^{2 \times L}$ 에서 시간축 길이)
* `--N-E`: Diehl–Cook 흥분 뉴런 수(시나리오 1.1, 1.2)
* `--N-hidden`: 준지도 실험(시나리오 2)의 히든 뉴런 수
* `--lif-tau-m` 등: LIF 파라미터들
* `--layer-index-scale`: $l_{\text{norm}}$ 스케일 조절 계수
* `--sup-input-encoding`: 완전지도 gradient mimic 시나리오(시나리오 3)의 입력 인코딩 방식 선택값 $\in \{\texttt{poisson}, \texttt{direct}\}$ (2.1.2절 참조)

### 8.3 정책·Critic 관련

* `--sigma-unsup1`: 시나리오 1.1 의 Gaussian 정책 표준편차
* `--sigma-unsup2`: 시나리오 1.2 의 Gaussian 정책 표준편차
* `--sigma-semi`: 시나리오 2 의 Gaussian 정책 표준편차
* `--sigma-sup`: 시나리오 3 의 Gaussian 정책 표준편차
* `--lr-actor`: Actor 학습률(필요 시 시나리오별 세분화 가능)
* `--lr-critic`: Critic 학습률
* `--ppo-eps`: PPO 클리핑 범위 $\epsilon$
* `--ppo-epochs`: 한 이미지 미니배치(여러 에피소드)에서 수집된 이벤트들에 대해 PPO 업데이트를 반복하는 epoch 수
* `--batch-size-images`: 이미지 단위 미니배치 크기(에피소드 수, 2.1절 및 2.9.3절 참조)
* `--events-per-image`: 각 이미지(에피소드)에서 저수지 샘플링으로 선택할 이벤트 수 상한 $K$ (2.9.3절의 per-image 이벤트 서브샘플링에서 사용)
* `--event-batch-size`: 서브샘플링으로 얻은 이벤트 집합 $\tilde{\mathcal E}$ 를 Actor·Critic에 넣을 때 사용할 **이벤트 미니배치 크기**. 너무 크게 잡으면 GPU 메모리 사용량이 증가하고, 너무 작게 잡으면 커널 런칭 오버헤드가 커질 수 있다. on-policy 특성은 유지되며, 2.9.4절의 미니배치 PPO 근사와 동일한 맥락이다.

### 8.4 비지도·준지도 보상 관련


* `--rho-target`: 비지도 희소성 목표 발화율 $\rho_{\text{target}}$
* `--alpha-sparse`: $R_{\text{sparse}}$ 가중치
* `--alpha-div`: $R_{\text{div}}$ 가중치
* `--alpha-stab`: $R_{\text{stab}}$ 가중치
* `--beta-margin`: 준지도 마진 보상 스케일 $\beta$
* `--exc-clip-min/max`: 완전 비지도 실험(실험 1, 2)에서 사용하는 흥분 자리 가중치 클리핑 범위
* `--inh-clip-min/max`: 완전 비지도 실험(실험 1, 2)에서 사용하는 억제 자리 가중치 클리핑 범위
* `--w-clip-min/max`: 준지도(실험 3) 및 완전지도 gradient mimicry(실험 4)에서 사용하는 전역 가중치 클리핑 범위. 모든 학습 시냅스에 대해 각 업데이트 후 $w_{i} \leftarrow \operatorname{clip}\bigl(w_{i}, \texttt{w-clip-min}, \texttt{w-clip-max}\bigr)$ 를 적용하여, 시냅스 가중치가 $[\texttt{w-clip-min}, \texttt{w-clip-max}]$ 구간을 벗어나지 않도록 강제한다.

### 8.5 gradient mimicry 관련

* `--alpha-align`: Teacher gradient 기준 업데이트 스케일 $\eta_{\text{align}}$
* `--log-gradient-stats`: gradient–$\Delta w$ 통계 저장 여부(텍스트 기반)

### 8.6 로깅·실험 관리

* `--seed`: 난수 시드
* `--num-epochs`: epoch 수 또는 에피소드 수
* `--log-interval`: 몇 에피소드마다 로그를 남길지
* `--run-name`: 실험 이름(결과 디렉토리 이름에 사용)

로그 파일 형식은 모두 텍스트 파일을 기본으로 하며, 실험 설계 상 JSON 형식의 로그 파일은 사용하지 않는다.

## 9. 한계점 및 신뢰할당 문제

마지막으로 이 설계에서 구조적으로 발생하는 한계점과 신뢰할당 문제를 정리한다.

1. 에피소드 단위 전역 보상에 따른 신뢰할당 문제 (실험 1–3)  
   완전 비지도/준지도 시나리오(실험 1–3)에서는 이미지 1장에 대해 전역 보상 $R(x)$ 하나만 계산하고,
   2.9.3절에서 설명한 per-image 저수지 샘플링을 거쳐 선택된 이벤트들에 동일한 리턴을 할당한다.
   이 때문에 어떤 이벤트가 보상을 개선 또는 악화시켰는지를 세밀하게 구분하기 어렵고,
   전형적인 Monte Carlo 기반 Actor–Critic 과 같은 신뢰할당 문제가 존재한다.
   MC 기반 PPO 클리핑을 도입해도 전역 보상 자체의 한계는 그대로 남는다.

2. gradient mimicry 의 부분적 완화 효과 (실험 4)  
   완전지도 시나리오(실험 4)에서는 Teacher gradient 를 도입해 개별 시냅스 업데이트 방향과 크기에 대한 정보를 직접 제공받고,
   per-event 보상 $R_e^{\text{sup}}$ 을 사용해 보다 로컬한 수준에서 보상을 할당한다.
   이로 인해 신뢰할당 문제가 상당 부분 완화되지만, surrogate gradient 자체의 한계와 noise 를 그대로 갖고 있고,
   여전히 PPO 의 MC 성격(에피소드 단위로 누적된 advantage 를 이용하는 구조) 때문에 완전히 사라지지는 않는다.

3. 로컬 상태 표현의 한계  
   로컬 상태 $z_{i}(t)$ 는 두 개의 스파이크 히스토리와 소수의 추가 feature 로만 구성된다. 전역 네트워크 상태나 장기적인 문맥을 관찰할 수 없으므로, 매우 긴 시간 스케일에서만 드러나는 효과나 여러 레이어 간 상호작용을 완벽히 포착하는 데에는 구조적 한계가 있다.

4. 표현력과 생물학적 그럴듯함 간 트레이드오프  
   CNN+MLP 기반 로컬 정책은 표현력이 높지만, 생물학적 plausibility 를 위해 입력을 로컬 스파이크 히스토리와 몇 개의 추가 feature 로 제한했다. 이로 인해 “전역 정보를 거의 보지 않는 로컬 규칙”이라는 제약은 유지되지만, 완전히 생물학적인 모델과는 여전히 거리가 있다.

5. 이상적인 시냅스 단위 on-policy RL 과의 차이  
   이론적으로는 **각 시냅스(또는 최소한 각 뉴런)가 자신의 pre/post 스파이크 히스토리만을 관찰하는 로컬 에이전트**가 되어, 스파이크가 발생할 때마다 on-policy 로 업데이트를 수행하는 **진짜 spike-by-spike 로컬 RL** 을 목표로 한다. 그러나 실제 구현에서는 메모리·연산량 제약 때문에 모든 이벤트마다 정책을 호출하고 가중치를 즉시 갱신하는 것이 현실적으로 불가능하다.  
   따라서 본 프로젝트에서는 에피소드 동안 가중치를 고정한 채 이벤트와 전역 보상을 관찰하고, 에피소드 종료 후에 **에피소드 단위 on-policy MC PPO** 를 통해 한 번에 가중치를 갱신하는 구조를 택한다. 이때 MDP 수준의 에이전트는 “개별 시냅스”가 아니라 **전체 가중치 벡터 $W$ 를 상태로 가지는 단일 에이전트(가중치 공간 MDP)** 로 해석된다.  
   그럼에도 로컬 정책의 입력 $z_{i}(t)$ 는 여전히 개별 시냅스/뉴런의 스파이크 히스토리와 로컬 feature 로만 구성되므로, **“단일 뉴런·시냅스 수준의 로컬 의사결정”을 전역 에이전트가 근사하는 구조**로 볼 수 있다. 즉, 이 설계는 진정한 시냅스 단위 on-policy RL 에 비해 계산적으로 완화된 근사이지만, 로컬 상태 표현을 유지함으로써 원래 목표했던 “로컬 학습 규칙의 RL 해석”이라는 철학을 가능한 한 보존한다는 점에서 한계와 타협점을 함께 가진다.