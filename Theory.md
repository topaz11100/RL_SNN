### 1. 연구 개요 및 목표

**1.1 전체 목표**

이 연구의 목표는 강화학습(Reinforcement Learning, RL) 에이전트를 이용해 스파이킹 신경망(Spiking Neural Network, SNN)의 **시냅스 가중치 업데이트 규칙(plasticity rule)** 을 학습시키는 것이다.

기존 STDP 식처럼 사람이 고정한 수식을 그대로 사용하는 대신, 각 시냅스를 **로컬 상태만 관측하는 RL 에이전트**로 보고 이 에이전트가

* 로컬 상태(local state)를 기반으로 **가중치 변경량(action)** 을 제안하고
* 전역 과제 성능 혹은 BPTT 기울기 등 **전역 신호를 보상(reward)** 으로 받아 학습

하도록 설계한다.

여기서 정책이 내놓는 액션은 **새 가중치 자체가 아니라 현재 가중치에 더해질 가중치 변화량**으로 해석한다. 즉 한 이벤트에서 정책 출력이 Δd라면, 실제 시냅스 가중치 업데이트는

* Δw = η · Δd
* w ← w + Δw

형태가 되며, Δd는 정규화된 "가중치 변화량"이다.

본 보고서는 이후 구현 보고서와 코드 구조의 기준이 되는 **고정된 실험 설계(구조, 타임스텝, 네트워크 아키텍처 포함)** 를 명시한다.

**1.2 실험 축**

실험은 기본적으로 두 축으로 구성하고, 이후 추가 시나리오를 확장한다.

1. **완전 비지도 설정에서의 구조·역할 창발(emergence)**
2. **지도 설정에서의 전역 기울기 근사(gradient mimicry)**

여기에 더해, 완전 비지도 설정의 **단일 정책 변형**과, 출력 라벨을 이용하는 **준지도 분류 시나리오**를 추가로 정의한다.

### 2. 공통 구성 요소

**2.1 데이터셋 및 입력 인코딩**

* 데이터셋

  * MNIST (28×28 회색조 이미지, 10 클래스)를 사용한다.

* 정규화

  * 각 픽셀 값을 [0, 1] 구간으로 정규화한다.

* 푸아송 인코더(Poisson encoder)

  * 각 픽셀 정규화 값 p ∈ [0, 1] 에 대해 주어진 시간 길이 T 동안 픽셀마다 독립적인 푸아송 스파이크 열을 생성한다.
  * 시뮬레이션은 이산 시간 스텝으로 진행되며, 한 스텝마다 해당 픽셀에서 스파이크가 발생할 확률을 p 에 비례하도록 설정한다.

* 타임스텝 설정

  * 완전 비지도 실험 1 (창발)

    * 한 입력당 **T_unsup = 100 타임스텝** 동안 스파이크를 주입한다.

  * gradient mimicry 실험 2 (지도)

    * 한 입력당 **T_sup = 16 타임스텝** 동안 스파이크를 주입한다.

  * 준지도 분류 실험 3

    * 한 입력당 **T_semi = 16 타임스텝** 동안 스파이크를 주입한다 (gradient mimicry 와 동일 길이 사용).

타임스텝 수 T_unsup, T_sup, T_semi 는 실험 전체에서 고정된 상수로 사용한다.

**2.2 뉴런 모델(LIF)**

모든 스파이킹 층은 **LIF(Leaky Integrate-and-Fire)** 뉴런을 사용한다.

* 연속 시간 형태

  * τ_m dV/dt = −(V − V_rest) + R I_in

* 이산 시간 구현

  * 시뮬레이션은 타임스텝 Δt 단위로 진행하며, 각 스텝마다 다음과 같이 업데이트한다.

    * V(t+1) = V(t) + (Δt/τ_m)·(−(V(t) − V_rest) + R I_in(t))

  * 발화 조건

    * V(t+1) ≥ V_th 가 되면 스파이크를 발생시키고 막전위를 V_reset 으로 리셋한다.

각 층별 τ_m, V_th, V_reset, R, Δt 등 하이퍼파라미터는 구현 단계에서 명시적으로 고정하여 사용한다.

**2.3 스파이크 히스토리 배열과 1D-CNN 입력 길이**

각 시냅스는 pre/post 스파이크 히스토리를 **최근 K 스텝 길이**로 유지하며 이를 정책 입력 전단 1D-CNN 의 입력으로 사용한다.

* 시뮬레이션 타임스텝 수 T 와 실제로 CNN 에 전달되는 스파이크 히스토리 배열 길이 K 는 서로 다른 값으로 설정될 수 있다. 예를 들어 시뮬레이션은 T 스텝 동안 수행하되 최근 K 스텝만 잘라서 정책 입력으로 사용할 수 있다.
* 히스토리 배열이 실제로 관측된 스파이크 시퀀스를 넘지 않도록 항상 **K ≤ T** 를 만족하도록 설정한다.

구체적인 실험에서는 다음과 같이 설정한다.

* 완전 비지도 실험 1

  * 타임스텝 수 T_unsup = 100

  * 스파이크 히스토리 배열 길이 K_unsup = 100 (기본값 일반적으로 1 ≤ K_unsup ≤ T_unsup)

  * 한 시냅스 i 의 입력 배열 크기

    * X_spike,i^(unsup)(t) ∈ R^(2×K_unsup)

  * 채널 2 는 각각 pre 뉴런 post 뉴런의 스파이크 시퀀스를 의미한다.

* gradient mimicry 실험 2

  * 타임스텝 수 T_sup = 16
  * 스파이크 히스토리 배열 길이 K_sup = 16 (기본값 일반적으로 1 ≤ K_sup ≤ T_sup)
  * 한 시냅스 i 의 입력 배열 크기

    * X_spike,i^(sup)(t) ∈ R^(2×K_sup)

* 준지도 분류 실험 3

  * 타임스텝 수 T_semi = 16
  * 스파이크 히스토리 배열 길이 K_semi = 16
  * 한 시냅스 i 의 입력 배열 크기

    * X_spike,i^(semi)(t) ∈ R^(2×K_semi)

현재 설계에서는 단순화를 위해 K_unsup = T_unsup, K_sup = T_sup, K_semi = T_semi 로 두지만 구현 측면에서는 T 와 K 를 독립적으로 조정하면서 항상 K ≤ T 제약만 유지하면 된다.

**2.4 1D-CNN 전단 구조(공통)**

스파이크 배열만 1D-CNN 으로 처리한다. CNN 전단은 모든 실험에서 공통으로 사용하며 시간 축 길이에 따라 입력 길이만 달라진다.

* 입력

  * 완전 비지도: 2×K_unsup 배열(기본값 2×100)
  * gradient mimicry: 2×K_sup 배열(기본값 2×16)
  * 준지도 분류: 2×K_semi 배열(기본값 2×16)

* 1D-CNN 구조

  1. Conv1

     * in_channels = 2
     * out_channels = 16
     * kernel_size = 5
     * padding = 2
     * stride = 1
     * 활성함수: ReLU

  2. Conv2

     * in_channels = 16
     * out_channels = 32
     * kernel_size = 5
     * padding = 2
     * stride = 1
     * 활성함수: ReLU

  3. Global average pooling (time 축 평균)

     * 출력 feature 벡터 h_i(t) 의 차원은 32

CNN 전단은 **스파이크 시간 패턴만** 보고 해당 시냅스 주변의 시계열 패턴을 요약한 feature 벡터 h_i(t) 를 생성한다.

**2.5 정책·가치 네트워크 입력 구성**

CNN 출력에 스칼라 정보를 concat 해서 MLP 에 넣는 **late fusion** 구조를 사용한다.

* 공통 스칼라 입력

  * 해당 시냅스의 현재 가중치 w_i(t)

* 레이어 번호 l_norm,i

  * gradient mimicry 실험 2 에서만 사용한다.
  * 해당 시냅스가 속한 레이어 인덱스를 0~1 구간으로 정규화한 값이다.
  * 완전 비지도 실험 1 과 준지도 실험 3 에서는 학습층이 사실상 단층 혹은 얕은 구조이므로 l_norm 을 로컬 상태에서 사용하지 않는다.

* 완전 비지도 실험 1 에서의 정책·가치 입력 벡터

  * h_i(t) ∈ R^32, w_i(t) ∈ R
  * z_i^(unsup)(t) = [h_i(t); w_i(t)] ∈ R^33

* gradient mimicry 실험 2 에서의 정책·가치 입력 벡터

  * h_i(t) ∈ R^32, w_i(t) ∈ R, l_norm,i ∈ R
  * z_i^(sup)(t) = [h_i(t); w_i(t); l_norm,i] ∈ R^34

* 준지도 분류 실험 3 에서의 정책·가치 입력 벡터

  * h_i(t) ∈ R^32, w_i(t) ∈ R
  * z_i^(semi)(t) = [h_i(t); w_i(t)] ∈ R^33

**2.6 정책 네트워크(Actor)**

정책 네트워크는 입력 벡터 z_i(t) 를 받아 **연속 스칼라 액션 Δd_i(t)** 를 생성한다. 이 액션은 "가중치 변화량"으로 해석되며 현재 가중치에 그대로 더해질 값(학습률 스케일을 곱한 후)이다.

* 입력 차원

  * 완전 비지도: 33
  * gradient mimicry: 34
  * 준지도 분류: 33

* 기본 MLP 구조(공통)

  1. FC1

     * input_size = 33 또는 34
     * hidden_size = 128
     * 활성함수: ReLU

  2. FC2

     * hidden_size = 128
     * 활성함수: ReLU

  3. FC3

     * hidden_size = 1
     * 활성함수: Tanh (출력을 [−1, 1] 범위로 제한)

* 정책의 확률적 형태 (Gaussian policy)

  * 네트워크 출력 m_i(t) = Tanh(·) ∈ [−1, 1] 를 **가우시안 정책의 평균**으로 사용한다.

  * 고정 표준편차 σ_policy 를 두고

    * Δd_i(t) ~ N(m_i(t), σ_policy^2)

  * σ_policy 는 모든 실험에서 공통으로 사용하며 예를 들어 σ_policy = 0.1 로 고정한다.

  * 샘플링된 Δd_i(t) 는 필요시 [−1, 1] 로 클리핑한다.

* 액션의 해석과 가중치 업데이트

  * 정책 액션은 정규화된 가중치 변화량으로 해석한다.

  * 학습률 η_ex, η_inh 등 시냅스 타입별 학습률을 둘 수 있으며 기본값은 η 로 통일한다.

  * 한 이벤트에서 시냅스 i 의 실제 가중치 변화는

    * Δw_i(t) = η_group(i) · Δd_i(t)
    * w_i(t+1) = clip(w_i(t) + Δw_i(t), w_min, w_max)

  * 여기서 group(i)는 시냅스 타입(예를 들어 입력→흥분, 억제→흥분 등)에 따라 다른 학습률을 쓸 수 있게 하는 인덱스이다.

* 파라미터 공유 구조

  * 기본적으로 각 실험은 **정해진 개수의 정책 네트워크**를 사용한다.

    * 완전 비지도 실험 1: 입력→흥분 가중치용 정책 π_exc 와 억제→흥분 가중치용 정책 π_inh 두 개를 사용한다.
    * 완전 비지도 단일 정책 실험 1′ 및 준지도 실험 3: 모든 학습 시냅스에 대해 **단일 정책 π_single** 을 사용한다.
    * gradient mimicry 실험 2: 모든 학습 시냅스에 대해 단일 정책 π_grad 를 사용한다.
  * 각 정책은 CNN 전단을 공유하고 FC head 만 별도로 가진 **멀티헤드 구조**로 구현할 수 있다.

**2.7 가치함수 네트워크(Critic)**

가치함수 네트워크는 동일한 입력 벡터 z_i(t) 를 받아 해당 상태에서의 **기대 누적 보상 값**을 추정한다.

* 입력 차원

  * 완전 비지도: 33
  * gradient mimicry: 34
  * 준지도 분류: 33

* MLP 구조 (정책 네트워크와 동급 혹은 약간 큰 용량으로 고정)

  1. FC1

     * input_size = 33 또는 34
     * hidden_size = 128
     * 활성함수: ReLU

  2. FC2

     * hidden_size = 128
     * 활성함수: ReLU

  3. FC3

     * hidden_size = 1
     * 활성함수: 없음 (선형 출력)

정책 네트워크와 비교했을 때 이전 버전(64 유닛)보다 **hidden_size 를 128 로 늘려** 충분한 표현력을 확보하고 정책 네트워크와 **동급 수준의 용량**을 갖도록 확정한다.

* 파라미터 공유 구조

  * 기본값은 **하나의 Critic V_φ** 를 모든 실험에서 공유하는 것이다.
  * 필요시 실험별로 서로 다른 Critic 을 둘 수 있으나 이 설계 문서에서는 구현 단순화를 위해 하나의 V_φ 를 공유한다고 명시한다.

* 출력

  * V_φ(z_i(t)) ∈ R
  * 완전 비지도 실험에서는 에피소드 전체 전역 보상 R 의 기댓값을
  * gradient mimicry 실험에서는 시간·시냅스별 보상 r_i,t 의 기대값을
  * 준지도 분류 실험에서는 정답 여부와 발화 패턴에 기반한 전역 보상의 기대값을

  에 대해 baseline 으로 사용한다.

CNN 전단은 정책과 가치함수가 공유하고 z_i(t) 이후의 FC head 만 Actor 와 Critic 이 별도로 갖도록 구현한다.

**2.8 RL 궤적 구조와 기호 정리**

이후 모든 실험에서 공통으로 사용하는 RL 표기와 궤적 구조를 미리 정리한다.

* 이벤트 인덱스

  * t: 시뮬레이션 시간 스텝 (1 ≤ t ≤ T)
  * i: 시냅스 인덱스
  * e: 에피소드 내에서의 이벤트 인덱스 (pre/post 스파이크가 실제로 발생한 시점들만 순서대로 나열)

* 상태·행동·보상

  * 상태(state)

    * s_e ≡ z_i(t): 이벤트 e 가 발생했을 때 시냅스 i 의 로컬 상태 벡터 (CNN feature + 스칼라 정보)

  * 행동(action)

    * a_e ≡ Δd_i(t): 정책이 제안한 정규화된 가중치 변화량

  * 보상(reward)

    * r_e: 이벤트 e 에 대응되는 보상

      * 완전 비지도 및 준지도에서는 보통 **에피소드 종료 후 전역 보상 R** 을 계산한 뒤 모든 이벤트에 대해 r_e = R 로 할당한다.
      * gradient mimicry 에서는 각 이벤트마다 g_i,t 와 Δw_agent 에 따라 r_e 를 개별적으로 계산한다.

* Critic 출력과 advantage

  * V_e ≡ V_φ(s_e)

  * 누적 보상(return)

    * G_e = Σ_{k ≥ e} γ^{k−e} r_k

  * advantage

    * A_e = G_e − V_e

  * 완전 비지도와 준지도에서는 γ_unsup = γ_semi = 1 로 설정해 G_e = R 이 되도록 단순화한다.

  * gradient mimicry 에서는 보상이 이벤트별로 이미 충분히 local 하므로 γ_grad = 1 로 두고 G_e = r_e 로 본다.

각 에피소드마다 다음 정보를 모아 RL 버퍼에 저장한다.

* (s_e, a_e, V_e, r_e) 의 시퀀스 (에피소드 내 모든 이벤트 e)

**2.9 Actor–Critic 업데이트 수식(공통)**

정책 경사상승은 **Gaussian 정책**에 대한 표준 Actor–Critic 형태로 정의한다.

* 정책의 로그 확률

  * 한 이벤트에서 정책 평균이 m_e, 액션이 a_e, 정책 표준편차가 σ_policy 일 때

    * log π_θ(a_e | s_e) = −(a_e − m_e)^2 / (2σ_policy^2) + C

  * C 는 θ 와 무관한 상수이므로 최적화에서 무시한다.

* Actor 손실 (gradient ascent 에 해당하는 부호 포함)

  * advantage 가 A_e 일 때 에피소드당 Actor 손실은

    * L_actor = − E_e [ A_e · log π_θ(a_e | s_e) ]

  * 구현에서는 에피소드 내 이벤트 e 에 대해 평균을 취해 근사한다.

* Critic 손실

  * L_critic = E_e [ (G_e − V_φ(s_e))^2 ]

* 전체 RL 손실

  * L_RL = L_actor + β_v L_critic − β_ent H(π)

    * β_v: value 손실 가중치 (예: β_v = 0.5)
    * β_ent: 엔트로피 정규화 가중치 (정책의 탐색성을 유지하기 위해 사용 예: β_ent = 0.01)
    * H(π): Gaussian 정책의 엔트로피

* 파라미터 업데이트

  * Actor 파라미터 θ 와 Critic 파라미터 φ 에 대해

    * θ ← θ − α_actor ∂L_actor/∂θ
    * φ ← φ − α_critic ∂L_critic/∂φ

  * 실제 구현에서는 L_RL 에 대한 합산 그래디언트를 사용하되 Actor 와 Critic 의 learning rate 를 다르게 둘 수 있다.

  * 최적화 알고리즘은 Adam 을 기본으로 사용한다.

요약하면 "정책 경사상승"은 **advantage 로 가중된 log π 의 마이너스를 최소화하는 것**으로 구현되며 위 식을 그대로 코드에서 사용한다.

### 3. 실험 1: 완전 비지도 설정에서의 구조·역할 창발

**3.1 목표**

완전 비지도(라벨, 기울기 등 지도 신호 없음) 설정에서 Diehl–Cook 스타일의 E/I 구조 위에 RL 기반 플라스틱 정책을 얹었을 때

1. 입력→흥분 및 억제→흥분 시냅스에 대해 **서로 다른 기능적 역할을 하는 두 개의 정책**이 자발적으로 분화하는지 (예: 하나는 특징 강화 중심 하나는 발화 안정화 중심)
2. 뉴런 집단 수준에서 일부 뉴런이 주로 양수 outgoing weight, 다른 뉴런이 주로 음수 outgoing weight 를 갖는 **E/I 유사 sign 구조**가 자연스럽게 나타나는지

를 관찰하는 것이 목적이다.

Diehl–Cook 구조 자체는 흥분성·억제성 뉴런 타입을 하드코딩하지만 **입력→흥분 및 억제→흥분 시냅스의 미세한 가중치 패턴**을 어떻게 조정할지는 RL 정책에 전적으로 맡긴다.

**3.2 SNN 구조(완전 비지도, Diehl–Cook 아키텍처)**

완전 비지도 실험에서 사용하는 SNN 구조는 Diehl & Cook 2015 의 모델 구조를 그대로 따른다.

* 입력층

  * MNIST 28×28 픽셀을 784 개의 입력 푸아송 스파이크 소스로 매핑한다.
  * 각 입력 샘플에 대해 T_unsup = 100 타임스텝 동안 스파이크를 주입한다.

* 흥분성 LIF 층 (E 층)

  * 뉴런 수: N_E = 400
  * 모든 뉴런은 LIF 뉴런으로 구현한다.

* 억제성 LIF 층 (I 층)

  * 뉴런 수: N_I = 400
  * 억제성 LIF 뉴런 또는 단순한 leaky unit 으로 구현할 수 있으나 여기서는 동일한 LIF 형태를 가정한다.

* 연결 구조

  * 입력 → 흥분 (Input → E)

    * 784×400 fully connected
    * 이 가중치가 **주요 학습 대상**이며 RL 정책 π_exc 가 업데이트를 담당한다.
    * 가중치는 양수 영역 [0, w_max^exc] 로 클리핑해 흥분성 시냅스임을 보장한다.

  * 흥분 → 억제 (E → I)

    * **1:1 연결**

      * j 번째 흥분 뉴런 → j 번째 억제 뉴런으로만 연결
      * Diehl–Cook 모델에서와 동일하게 고정된 양수 가중치 w_EI 를 사용하며 학습하지 않는다.

  * 억제 → 흥분 (I → E)

    * 각 억제 뉴런 j 가 모든 흥분 뉴런 k 에 연결되는 **전역 억제(all-to-all)** 구조를 사용한다.
    * 자기 자신으로의 연결(j = k)에 대해서는 0 또는 별도의 값으로 두되 기본값은 0 으로 둔다 (원 논문과 동일한 선택).
    * 이 가중치는 음수 영역 [w_min^inh, 0] 에 클리핑해 억제성을 유지한다.
    * I → E 가중치는 RL 정책 π_inh 가 조정하는 두 번째 학습 대상이다.

요약하면 완전 비지도 실험 1 에서는 Diehl–Cook 의 **입력(784) → 흥분(400) ↔ 억제(400)** 구조를 그대로 사용하되

* Input → E
* I → E

두 타입의 시냅스에 대해 각각 별도의 정책(π_exc, π_inh)을 부여해 **두 개의 로컬 플라스틱 규칙이 서로 다른 역할을 학습하는지**를 관찰한다.

**3.3 순전파(에피소드 시뮬레이션)**

한 에피소드는 MNIST 입력 하나에 대응하며 순전파는 다음과 같이 진행된다.

1. 입력 이미지 x 를 [0, 1] 로 정규화하고 각 픽셀마다 푸아송 스파이크 소스를 초기화한다.

2. 모든 LIF 뉴런(E 층, I 층)의 막전위와 리셋 상태를 초기화한다.

3. t = 1,…,T_unsup 에 대해

   1. 입력층에서 푸아송 샘플링으로 스파이크 벡터 s_in(t) 를 생성한다.
   2. Input → E 가중치와 E → I, I → E 가중치를 이용해 E 층과 I 층의 입력 전류 I_in 을 계산한다.
   3. E 층, I 층의 막전위를 LIF 식으로 업데이트하고 임계치를 넘는 뉴런은 스파이크를 발생시킨다.
   4. 각 시냅스별로 pre/post 스파이크를 스파이크 히스토리 배열 X_spike,i^(unsup)(t) 에 기록한다.

4. T_unsup 스텝이 끝나면 각 흥분 뉴런의 발화 횟수를 집계해 winner 패턴 등을 계산한다.

이 순전파 동안 **pre 또는 post 스파이크가 발생한 타임스텝**에서만 RL 정책을 호출해 행위를 선택한다 (자세한 내용은 3.5 에서).

**3.4 로컬 상태와 정책 입력(완전 비지도)**

* 스파이크 히스토리 배열

  * 각 시냅스 i 는 pre/post 스파이크 히스토리를 길이 K_unsup 의 배열로 유지한다 (기본 설정 K_unsup = 100).
  * CNN 입력 크기: X_spike,i^(unsup)(t) ∈ R^(2×K_unsup).

* 정책·가치 입력 벡터

  * CNN 전단을 통과하여 feature 벡터 h_i(t) ∈ R^32 를 얻는다.

  * 현재 가중치 w_i(t) 를 concat 해

    * z_i^(unsup)(t) = [h_i(t); w_i(t)] ∈ R^33

  * 이 벡터를 정책 네트워크와 가치함수 네트워크가 공통 입력으로 사용한다.

* 정책 분할

  * Input → E 시냅스: π_exc 에 z_i^(unsup)(t) 를 넣어 액션을 샘플링한다.
  * I → E 시냅스: π_inh 에 z_i^(unsup)(t) 를 넣어 액션을 샘플링한다.

두 정책 모두 CNN 전단은 공유하고 FC head 만 다르게 가진다.

**3.5 전역 비지도 보상**

전역 보상 R 은 Diehl–Cook 스타일의 스파스하고 안정적인 특징 맵을 유도하기 위해 다음 세 항의 가중합으로 정의한다.

* R = α_sparse R_sparse + α_div R_div + α_stab R_stab

* R_sparse

  * 각 입력 자극에 대해 발화한 흥분 뉴런 수가 적을수록 (스파스할수록) 높은 값을 주는 항이다.

* R_div

  * 데이터셋 전체에 걸쳐 다양한 뉴런이 winner 로 선택될수록 높은 값이 되도록 정의한다 (특정 뉴런만 계속 이기지 않도록).

* R_stab

  * 동일 입력을 반복 제시했을 때 winner 패턴이 안정적일수록 높은 값이 되도록 정의한다.

구체적인 수식(정규화 방식 등)은 구현 단계에서 고정한다. 이 실험에서는 **에피소드 단위 전역 보상**만 사용하고 시간별 보상은 사용하지 않는다.

**3.6 RL 궤적 수집 방식(완전 비지도)**

완전 비지도 실험 1 에서 에피소드 하나는 다음과 같이 처리된다.

1. 모든 시냅스 가중치와 정책·가치 네트워크 파라미터를 초기화한다 (또는 학습 중간값에서 이어서 시작).

2. 에피소드 시작 시 스파이크 히스토리 배열과 LIF 막전위를 초기화한다.

3. t = 1,…,T_unsup 에 대해 순전파를 수행하면서

   1. 각 시냅스 i 에 대해 pre 또는 post 뉴런 중 하나라도 t 시점에 스파이크를 내면 **이벤트 e = (i, t)** 로 간주한다.
   2. 해당 시냅스 i 의 스파이크 히스토리 배열 X_spike,i^(unsup)(t) 을 갱신하고 CNN 전단을 통과시켜 h_i(t) 를 계산한다.
   3. z_i^(unsup)(t) = [h_i(t); w_i(t)] 를 구성한다.
   4. Input → E 이면 π_exc, I → E 이면 π_inh 에 z_i^(unsup)(t) 를 넣어 평균 m_e 를 얻고

      * Δd_e ~ N(m_e, σ_policy^2)

      를 샘플링한다.
   5. 액션 Δd_e 에 대해

      * Δw_e = η_group(i) · Δd_e
      * w_i ← clip(w_i + Δw_e, w_min_group(i), w_max_group(i))

      로 가중치를 즉시 갱신한다.
   6. Critic 에서 V_e = V_φ(z_i^(unsup)(t)) 를 계산한다.
   7. 이 시점에서는 전역 보상 R 을 아직 모르므로 보상은 비워둔 채로

      * (s_e, a_e, V_e) = (z_i^(unsup)(t), Δd_e, V_e)

      를 에피소드 버퍼에 저장한다.

4. t = T_unsup 까지 진행한 뒤 3.5 에 따라 전역 보상 R 을 계산한다.

5. 에피소드 버퍼에 저장된 모든 이벤트 e 에 대해 r_e = R 로 설정한다. (γ_unsup = 1 이므로 G_e = R.)

6. 2.9 의 공통 식을 사용해 Actor–Critic 업데이트를 수행한다.

이 과정에서 **RL 궤적은 "시냅스 이벤트" 단위로 수집**되며 한 에피소드 안에서 수천 개의 이벤트가 발생할 수 있다. 모든 이벤트가 같은 전역 보상 R 을 공유하되 각 이벤트의 advantage A_e 는 Critic 의 출력에 따라 달라진다.

**3.7 분석 항목**

완전 비지도 실험 1 에서는 다음 항목을 중점적으로 분석한다.

1. 정책별 STDP 커널 Δw(Δt) 추출

   * pre/post 스파이크 시간차 Δt 에 따른 평균 Δw 패턴을 추정해 π_exc, π_inh 가 각각 어떤 STDP 형태를 학습했는지 시각화한다.
2. 뉴런별 outgoing weight sign 분포 분석

   * 각 흥분 뉴런에서 나가는 가중치(특히 Input → E, E → I, I → E 의 합성 효과)를 분석해 일부 뉴런이 주로 양수 outgoing weight 를 갖고 일부는 음수 성분이 강한지 등 **E/I 유사 sign 구조**가 형성되는지 본다.
3. 두 정책의 역할 분화

   * π_exc 와 π_inh 가 각각 강화 중심, 억제·안정화 중심 등 서로 다른 기능적 역할을 하도록 학습되는지 통계 분석을 수행한다.
4. minimal bias 조건과 scaffold 조건 비교

   * 억제 scaffold 의 세기나 고정 가중치의 존재 여부를 바꾸면서 창발 패턴이 어떻게 달라지는지 비교한다.

### 4. 실험 2: 전역 기울기 모방(gradient mimicry)

**4.1 목표**

지도 학습 환경에서 로컬 SNN 플라스틱 정책이 **전역 BPTT 기울기**를 어느 정도까지 근사할 수 있는지 정량적으로 평가한다.

* 기준선 모델: surrogate gradient 기반 BPTT 로 학습한 다층 SNN
* 로컬 정책이 제안하는 업데이트 Δw_agent 가

  * 전역 기울기 g = ∂L/∂w 와 얼마나 잘 정렬되는지 (sign 일치, 코사인 유사도 등)
  * 실제 성능(정확도, loss 수렴 속도)에서 기준선과 얼마나 가까운지

를 측정한다.

**4.2 SNN 구조(gradient mimicry)**

gradient mimicry 실험에서 사용하는 SNN 구조는 다음과 같다.

* 입력층

  * MNIST 28×28 픽셀을 784 개의 입력 푸아송 스파이크 소스로 매핑한다.
  * 각 입력 샘플에 대해 T_sup = 16 타임스텝 동안 스파이크를 주입한다.

* 히든 LIF 층

  * 히든층 1: 256 뉴런
  * 히든층 2: 128 뉴런
  * 히든층 3: 64 뉴런
  * 히든층 4: 32 뉴런

* 출력층

  * 10 클래스 분류를 위한 출력 뉴런 10개 (LIF 뉴런)

입력층을 제외하면 LIF 히든층 4개와 출력층 1개가 존재한다. 각 층 사이의 시냅스 가중치는

* 기준선 SNN 에서는 surrogate gradient BPTT 로
* 로컬 정책 SNN 에서는 Actor–Critic 으로

업데이트된다.

**4.3 로컬 상태와 정책 입력(gradient mimicry)**

* 스파이크 히스토리 배열

  * 각 시냅스 i 는 pre/post 스파이크 히스토리를 길이 K_sup 의 배열로 유지한다 (기본 설정 K_sup = 16).
  * CNN 입력 크기: X_spike,i^(sup)(t) ∈ R^(2×K_sup).

* 레이어 번호 l_norm,i

  * 입력층 다음 히든층 1을 0.2, 히든층 2를 0.4, 히든층 3을 0.6, 히든층 4를 0.8, 출력층을 1.0 과 같이 정규화하여 사용한다.

* 정책·가치 입력 벡터

  * CNN 전단을 통과하여 feature 벡터 h_i(t) ∈ R^32 를 얻는다.

  * 현재 가중치 w_i(t) 와 l_norm,i 를 concat 해

    * z_i^(sup)(t) = [h_i(t); w_i(t); l_norm,i] ∈ R^34

  * 이 벡터를 정책 네트워크 π_grad 와 Critic V_φ 가 공통 입력으로 사용한다.

**4.4 순전파 및 BPTT 역전파**

gradient mimicry 실험에서 한 에피소드(입력 하나)의 순전파·역전파는 다음과 같이 진행된다.

1. 입력 이미지를 푸아송 인코딩해 T_sup 타임스텝 동안 스파이크를 주입하며 SNN 을 시뮬레이션한다.

2. 각 시냅스 i, 각 타임스텝 t 에 대해

   * pre/post 스파이크를 X_spike,i^(sup)(t) 에 기록하고
   * 이벤트가 발생한 경우 z_i^(sup)(t) 를 구성해 정책에서 액션을 샘플링하고 가중치를 업데이트하며 (4.5 에서 자세히 기술)
   * 추후 BPTT 를 위해 막전위와 스파이크를 모두 저장한다.

3. T_sup 스텝이 끝나면 출력층의 스파이크 카운트를 s_k 를 집계하고 평균 발화율 r_k = s_k / T_sup 를 계산한다.

4. 정답 라벨 y 에 대해 cross entropy 형태의 분류 loss L_sup 를 정의한다.

   * 예를 들어 softmax(α r_k) 를 사용해 확률을 만들고 표준 cross entropy 를 취한다.

5. surrogate gradient 를 사용해 **시간에 걸친 BPTT** 를 수행하고 각 시냅스·시간에 대한 기울기 g_i,t = ∂L_sup/∂w_i(t) 를 계산한다.

이렇게 얻은 g_i,t 를 이용해 RL 보상을 구성하는 것이 gradient mimicry 의 핵심이다.

**4.5 로컬 보상 설계: gradient 정렬 기반 보상**

로컬 정책 학습에는 전역 기울기 g_i,t 를 Teacher 신호로 사용하는 **gradient 정렬 기반 보상**을 사용한다.

* 한 이벤트 e = (i, t) 에서 정책이 제안한 실제 가중치 변화량을 Δw_agent,i,t 라 할 때 (Δw_agent,i,t = η · Δd_i(t)) 전역 기울기 g_i,t 에 대해

  * r_i,t^grad = − g_i,t · Δw_agent,i,t

  는 "기울기와 같은 방향으로 업데이트하면 보상이 높아지도록" 설계된 항이다.

* 너무 큰 업데이트를 방지하기 위해 정규화 항을 포함한다.

  * r_i,t^total = − g_i,t · Δw_agent,i,t − λ · (Δw_agent,i,t)^2

  * λ 는 예를 들어 λ = 0.01 정도의 작은 양수로 고정한다.

각 이벤트 e 에 대해 r_e = r_i,t^total 로 두고 discount factor γ_grad = 1 로 설정해 G_e = r_e 로 본다. 즉 **각 이벤트의 보상은 그 이벤트에서의 gradient 정렬 품질만 반영**한다.

**4.6 RL 궤적 수집 및 Actor–Critic 업데이트(gradient mimicry)**

gradient mimicry 실험에서 한 에피소드의 처리 흐름은 다음과 같다.

1. 에피소드 시작 시 스파이크 히스토리와 막전위를 초기화한다.

2. t = 1,…,T_sup 동안 순전파를 수행하면서

   1. 각 시냅스 i 에 대해 pre/post 스파이크가 발생하면 이벤트 e = (i, t) 를 생성한다.
   2. X_spike,i^(sup)(t) 를 CNN 전단에 통과시켜 h_i(t) 를 얻고 z_i^(sup)(t) 를 구성한다.
   3. 정책 π_grad 에서 평균 m_e 와 액션 Δd_e 를 샘플링하고 실제 가중치 변화 Δw_e 를 적용한다.
   4. Critic 으로부터 V_e = V_φ(z_i^(sup)(t)) 를 계산한다.
   5. (s_e, a_e, V_e) 를 버퍼에 저장한다.

3. T_sup 이후 BPTT 로 각 이벤트 e 에 대응되는 g_i,t 를 계산하고 r_e = r_i,t^total 을 구성한다.

4. 모든 이벤트 e 에 대해 G_e = r_e, A_e = G_e − V_e 를 계산한다.

5. 2.9 의 공통 식을 사용해 L_actor, L_critic 을 계산하고 Actor–Critic 업데이트를 수행한다.

이 실험에서 RL 은 **기울기 방향을 맞추는 policy 를 학습하는 보조 학습자**로 동작한다. 최종 분석에서는

* g_i,t 와 Δw_agent,i,t 의 sign 일치 비율
* 두 벡터 사이의 코사인 유사도
* 분류 정확도와 loss 곡선 비교

등을으로 gradient mimicry 정도를 평가한다.

**4.7 분석 항목**

1. 기준선 BPTT 모델과 로컬 정책 모델의 학습 곡선 및 최종 정확도 비교
2. 여러 시점에서 각 시냅스에 대해 g_i,t 와 Δw_agent,i,t 의 상관계수 코사인 유사도 부호 일치 비율을 계산해 시간에 따른 정렬도 변화를 확인
3. 레이어 번호 l_norm 에 따른 전략 차이 분석 (깊이에 따른 다른 업데이트 패턴이 학습되는지)
4. l_norm 을 제거한 순수 로컬 버전과의 비교 실험으로 레이어 정보가 gradient mimicry 에 얼마나 기여하는지 평가

### 5. 추가 실험 시나리오

**5.1 실험 1′: 완전 비지도 단일 정책 버전**

이 시나리오는 실험 1 과 동일한 Diehl–Cook 아키텍처와 전역 비지도 보상을 사용하되 **정책 구조만 바꾸는 ablation** 이다.

* 구조 및 상태

  * SNN 구조, LIF 파라미터, 스파이크 히스토리, z_i^(unsup)(t) 의 정의는 3 장과 동일하다.

* 정책 구조의 차이

  * 실험 1에서는 Input → E 에 π_exc, I → E 에 π_inh 두 개의 정책을 사용했다.
  * 실험 1′에서는 **모든 학습 시냅스(Input → E 와 I → E)** 에 대해 하나의 정책 π_single 만 사용한다.
  * CNN 전단은 동일하고 FC head 도 하나만 존재한다.

* 액션 범위

  * 실험 1에서는 안정성을 위해 액션을 추가로 스케일링해 Δw_e = η · α_unsup · Δd_e (예: α_unsup = 0.2) 형태로 사용한다고 가정한다.

  * 실험 1′에서는 **스케일링 없이 Δd_e 의 전체 범위 [−1, 1]을 허용**한다.

    * Δw_e = η · Δd_e

  * 따라서 실험 1′은 **단일 정책 + 더 큰 업데이트 범위**를 사용하는 완전 비지도 실험이다.

* 순전파 및 학습 절차

  * 순전파와 RL 궤적 수집 방법은 3.3, 3.6 과 동일하다.
  * 차이는 "어떤 정책을 호출하는지" 뿐이며 모든 이벤트가 π_single 을 사용한다.
  * 전역 보상 R 계산과 Actor–Critic 업데이트는 3.5, 2.9 와 동일하다.

이 시나리오를 통해

* 정책을 둘로 나누어 역할을 분화시킬 필요가 있는지
* 단일 정책이 큰 액션 범위에서 얼마나 안정적으로 E/I 구조를 유지할 수 있는지

를 비교 평가할 수 있다.

**5.2 실험 3: 준지도 단일 정책 분류 시나리오**

이 시나리오는 **라벨 정보를 보상으로만 사용하는 RL 기반 분류** 설정이다. 전역 기울기를 직접 사용하지 않으므로 gradient mimicry 와 달리 BPTT 는 Teacher 로만 쓰지 않고 RL 로만 학습한다.

**5.2.1 SNN 구조(준지도)**

* 입력층

  * MNIST 28×28 픽셀을 784 개의 입력 푸아송 스파이크 소스로 매핑한다.
  * 각 입력 샘플에 대해 T_semi = 16 타임스텝 동안 스파이크를 주입한다.

* 히든 LIF 층

  * 히든층 1: 256 뉴런
  * 히든층 2: 128 뉴런

* 출력층

  * 10 클래스 분류를 위한 출력 LIF 뉴런 10개
  * **뉴런 번호와 라벨 매핑**은 사전에 고정한다.

    * 예: 출력층의 k 번째 뉴런이 숫자 k 를 의미 (필요시 0–9 또는 1–10 등 어떤 convention 이든 문서에 한 번 고정).

* 학습되는 가중치

  * Input → Hidden1, Hidden1 → Hidden2, Hidden2 → Output 의 모든 시냅스가 **단일 정책 π_semi** 의 제어를 받는다.
  * 각 시냅스의 로컬 상태와 z_i^(semi)(t) 는 2.5 의 정의를 따른다.

**5.2.2 순전파 및 출력 해석**

준지도 시나리오에서 한 에피소드는 (입력 x, 라벨 y) 한 쌍에 대응한다.

1. x 를 푸아송 인코딩해 T_semi 스텝 동안 SNN 에 주입한다.

2. t = 1,…,T_semi 동안

   * 각 층의 LIF 뉴런을 업데이트하고 스파이크를 기록한다.
   * 각 시냅스 i 에서 pre/post 스파이크가 발생하면 이벤트 e = (i, t) 로 간주하고 5.2.4 의 절차에 따라 RL 궤적을 수집한다.

3. 에피소드 종료 후 출력층 뉴런 k 의 스파이크 수 s_k 와 발화율 r_k = s_k / T_semi 를 계산한다.

4. 예측 라벨 y_hat 은

   * y_hat = argmax_k r_k

로 정의한다.

즉 이 시나리오에서 "모델 출력"은 **출력층 각 뉴런의 발화율**이다.

**5.2.3 보상 설계(분류 정확도 기반)**

라벨 y 와 예측 y_hat 을 이용해 단순하면서도 신뢰도 정보를 반영하는 보상을 설계한다.

* 기본 분류 보상

  * R_cls = +1 if y_hat = y
  * R_cls = −1 if y_hat ≠ y

* margin 기반 신뢰도 보상

  * 정답 뉴런의 발화율과 가장 많이 발화한 오답 뉴런의 차이를 margin 으로 둔다.

    * margin = r_y − max_{k ≠ y} r_k

  * margin 은 [−1, 1] 범위에 근사하므로 다음과 같이 추가 보상을 준다.

    * R_margin = α_margin · margin
    * 예: α_margin = 0.5

* 스파이크 절약 보상(선택 사항)

  * 전체 출력 스파이크 수를 줄이기 위해

    * R_sparse_out = − α_spike · (Σ_k s_k) / (T_semi · 10)
    * 예: α_spike = 0.01

* 최종 전역 보상

  * R = R_cls + R_margin + R_sparse_out

한 에피소드의 모든 이벤트 e 에 대해 r_e = R 로 두고 γ_semi = 1 로 설정해 G_e = R 로 본다.

**5.2.4 RL 궤적 및 Actor–Critic 업데이트(준지도)**

준지도 분류 시나리오에서 에피소드 하나의 처리 흐름은 다음과 같다.

1. 에피소드 시작 시 모든 뉴런 상태와 스파이크 히스토리 배열을 초기화한다.

2. t = 1,…,T_semi 동안 순전파를 수행하면서

   1. 각 시냅스 i 에 대해 pre 또는 post 스파이크가 발생하면 이벤트 e = (i, t) 를 생성한다.
   2. X_spike,i^(semi)(t) 를 CNN 전단에 통과시켜 h_i(t) 를 얻고 z_i^(semi)(t) = [h_i(t); w_i(t)] 를 구성한다.
   3. 단일 정책 π_semi 에서 평균 m_e 와 액션 Δd_e 를 샘플링하고

      * Δw_e = η_semi · Δd_e
      * w_i ← clip(w_i + Δw_e, w_min, w_max)

      로 가중치를 업데이트한다.
   4. Critic 으로부터 V_e = V_φ(z_i^(semi)(t)) 를 계산한다.
   5. (s_e, a_e, V_e) 를 에피소드 버퍼에 저장한다.

3. T_semi 이후 5.2.3 에 따라 전역 보상 R 을 계산하고 모든 이벤트에 r_e = R 를 할당한다.

4. 모든 이벤트 e 에 대해 G_e = R, A_e = G_e − V_e 를 계산한다.

5. 2.9 의 공통 식을 사용해 L_actor, L_critic 을 계산하고 Actor–Critic 업데이트를 수행한다.

이 시나리오는 **레이블 정보를 오직 보상 신호로만 사용하는 순수 RL 기반 분류**로 해석할 수 있으며

* gradient mimicry (기울기를 직접 Teacher 로 사용하는 지도 강화)
* 완전 비지도 (발화 통계 기반 보상만 사용)

사이의 "준지도"적인 중간 지점을 형성한다.

### 6. 기대 기여 및 활용

**6.1 실험 1 및 1′ (완전 비지도)**

* Diehl–Cook E/I 구조 위에서 전역 비지도 보상만으로도 두 개의 로컬 플라스틱 정책이 서로 다른 기능적 역할로 분화하는지 확인할 수 있다.
* 단일 정책 버전(실험 1′)과 비교해

  * 정책을 나누는 것이 실제로 구조·역할 창발에 필요하거나 유리한지
  * 액션 범위 제약(α_unsup) 이 안정성과 표현력에 어떤 영향을 미치는지

를 정량적으로 평가할 수 있다.

**6.2 실험 2 (gradient mimicry)**

* 로컬 SNN 플라스틱 규칙이 다층 SNN 에서 전역 BPTT 기울기를 어느 정도까지 근사할 수 있는지
* 깊이에 따른 gradient 패턴(레이어 정보 l_norm 포함)을 얼마나 잘 모사하는지
* 순수 BPTT 모델과 비교했을 때 정확도·수렴 속도·안정성에서 어느 정도 격차가 나는지

를 정량적으로 분석할 수 있다.

**6.3 실험 3 (준지도 분류)**

* 정답 라벨만을 이용한 간단한 보상 설계로도 RL 기반 플라스틱 정책이 분류 문제를 어느 정도 해결할 수 있는지 확인할 수 있다.
* gradient 를 직접 사용하지 않는 순수 RL 설정에서

  * 로컬 상태 기반 정책이 출력 발화율을 어떻게 조절해 가는지
  * BPTT 기반 모델 대비 성능·샘플 효율·안정성에서 어떤 특징을 보이는지

를 비교할 수 있다.

이 전체 설계는 이후 구현 보고서와 코드 구조의 기준으로 사용되며 각 시나리오별로 **순전파·역전파·RL 궤적 수집·보상 계산·정책 경사상승**이 어떻게 이루어지는지를 명확히 고정하기 위한 것이다.
